{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#thrml-boost","title":"THRML-Boost","text":"<p>Performance-optimized block Gibbs sampling for probabilistic graphical models in JAX.</p> <p>THRML-Boost is a fork of Extropic AI's THRML library that targets the JAX compilation and runtime bottlenecks in the original implementation. The API is unchanged \u2014 it's a drop-in replacement.</p> <p>The library provides GPU-accelerated tools for blocked Gibbs sampling on sparse, heterogeneous graphs. It's a good fit for Ising models, Boltzmann machines, discrete energy-based models, or anything with a bipartite factor-graph structure.</p> <p>What's different from upstream THRML:</p> <ul> <li>Parallel tempering chains run via <code>jax.vmap</code> instead of a Python loop \u2014 constant compile time, better GPU utilization</li> <li>Global state threaded through <code>jax.lax.scan</code> carry \u2014 no redundant rebuilds each iteration</li> <li>Moment accumulator dtype fixed at construction \u2014 avoids silent float64 promotion on GPU</li> <li><code>BlockSpec</code> pre-built and reused in energy evaluation \u2014 eliminates per-call reconstruction</li> <li>Deterministic global state ordering \u2014 reproducible across runs</li> </ul> <p>See the architecture guide for how the internals work, or jump straight to the API reference.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python \u2265 3.10 and a working JAX installation.</p> <pre><code>git clone https://github.com/dek3rr/thrml-boost.git\ncd thrml-boost\npip install -e .\n</code></pre> <p>For notebooks and examples:</p> <pre><code>pip install -e \".[examples]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick example","text":"<p>Sample a small Ising chain with two-color block Gibbs:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom thrml_boost import SpinNode, Block, SamplingSchedule, sample_states\nfrom thrml_boost.models import IsingEBM, IsingSamplingProgram, hinton_init\n\nnodes = [SpinNode() for _ in range(5)]\nedges = [(nodes[i], nodes[i + 1]) for i in range(4)]\nbiases = jnp.zeros((5,))\nweights = jnp.ones((4,)) * 0.5\nbeta = jnp.array(1.0)\nmodel = IsingEBM(nodes, edges, biases, weights, beta)\n\nfree_blocks = [Block(nodes[::2]), Block(nodes[1::2])]\nprogram = IsingSamplingProgram(model, free_blocks, clamped_blocks=[])\n\nkey = jax.random.key(0)\nk_init, k_samp = jax.random.split(key, 2)\ninit_state = hinton_init(k_init, model, free_blocks, ())\nschedule = SamplingSchedule(n_warmup=100, n_samples=1000, steps_per_sample=2)\n\nsamples = sample_states(k_samp, program, schedule, init_state, [], [Block(nodes)])\n</code></pre>"},{"location":"#attribution","title":"Attribution","text":"<p>THRML-Boost is a derivative work of thrml by Extropic AI, licensed under the Apache License 2.0. See the NOTICE file for details.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#what-is-thrml","title":"What is THRML?","text":"<p>THRML is a JAX-based Python library for efficient block Gibbs sampling of graphical models at scale. It provides the machinery for blocked Gibbs sampling on any graphical model and includes built-in support for Boltzmann Machines and other discrete energy-based models.</p> <p>THRML was originally developed by Extropic AI. THRML-Boost is a performance-optimized fork that preserves the full API while improving JAX compilation and runtime efficiency.</p>"},{"location":"architecture/#core-concepts","title":"Core concepts","text":"<p>From a user perspective there are three main components: blocks, factors, and programs. For worked examples see the example notebooks.</p>"},{"location":"architecture/#blocks","title":"Blocks","text":"<p>A <code>Block</code> is a collection of nodes of the same type with implicit ordering. Blocks are the unit of parallelism in block Gibbs sampling \u2014 all nodes in a block are updated simultaneously in a single SIMD-friendly JAX operation.</p>"},{"location":"architecture/#factors","title":"Factors","text":"<p>Factors organize interactions between variables into a bipartite factor graph. Each factor synthesizes a batch of <code>InteractionGroup</code>s and must implement <code>to_interaction_groups()</code>. An <code>InteractionGroup</code> specifies directed computational dependencies: which nodes to update (head), which neighbor states to read (tail), and what static parameters (weights) to use.</p>"},{"location":"architecture/#programs","title":"Programs","text":"<p>Programs are the orchestrating data structures. <code>BlockSamplingProgram</code> handles the mapping and bookkeeping for padded block Gibbs sampling \u2014 managing global state representations efficiently for JAX. <code>FactorSamplingProgram</code> is a convenience wrapper that converts factors into interaction groups automatically. Programs coordinate free/clamped blocks, conditional samplers, and interactions to execute the sampling loop.</p>"},{"location":"architecture/#internal-design","title":"Internal design","text":"<p>The core approach is to represent everything as contiguous arrays and PyTrees, operate on these flat structures, and map to/from them at the user boundary. Internally this is the \"global state\" (as opposed to the user-facing \"block state\"). This is similar in spirit to struct-of-arrays (SoA) layout and to other JAX graphical model packages like PGMax.</p> <p>An important distinction from PGMax is that THRML supports PyTree states and heterogeneous node types. Heterogeneity is handled by splitting nodes according to their PyTree structure and organizing the global state as a list of these PyTrees, stacked across blocks that share the same structure. The management of these indices and the mapping between block and global representations is constructed and held by the program's <code>BlockSpec</code>.</p> <p>Since JAX does not support ragged arrays, every block within a structural group must have the same leaf array size. THRML handles variable block sizes by stacking and padding. There is an inherent tradeoff: padding wastes some compute, but the alternative \u2014 looping over blocks in Python \u2014 incurs untenable XLA compile-time cost.</p> <p>Everything else exists to provide convenience for creating and working with a program. With a tight core focused on block index management and padding, the codebase stays lightweight and hackable.</p>"},{"location":"architecture/#what-thrml-boost-optimizes","title":"What THRML-Boost optimizes","text":"<p>THRML-Boost does not change the mathematical semantics of any sampler. It targets the JAX compilation and runtime overhead:</p> <ul> <li><code>jax.vmap</code> parallel tempering \u2014 replaces the Python for-loop over chains that unrolled N copies of the full Gibbs graph into XLA. One kernel, all chains, constant compile time.</li> <li><code>jax.lax.scan</code> carry threading \u2014 global state is now carried through the scan loop instead of being rebuilt from block states on every iteration.</li> <li>Fixed accumulator dtype \u2014 <code>MomentAccumulatorObserver</code> no longer infers dtype per step; set once at construction (float32 default).</li> <li>Pre-built <code>BlockSpec</code> passthrough \u2014 <code>energy()</code> accepts a pre-built spec to avoid reconstructing it on every call (critical during tempering swap attempts).</li> <li>Deterministic global state ordering \u2014 <code>dict.fromkeys()</code> replaces <code>set()</code> for deduplication, so ordering is reproducible across runs.</li> <li>Ragged <code>hinton_init</code> \u2014 correctly handles blocks of different sizes.</li> </ul>"},{"location":"architecture/#class-hierarchy","title":"Class hierarchy","text":""},{"location":"architecture/#factors_1","title":"Factors","text":"<pre><code>AbstractFactor\n\u251c\u2500\u2500 WeightedFactor\n\u2502   \u2514\u2500\u2500 DiscreteEBMFactor \u2014 spin \u00d7 categorical polynomial interactions\n\u2502       \u251c\u2500\u2500 SquareDiscreteEBMFactor \u2014 merged groups for square weight tensors\n\u2502       \u2502   \u251c\u2500\u2500 SpinEBMFactor \u2014 spin-only\n\u2502       \u2502   \u2514\u2500\u2500 SquareCategoricalEBMFactor \u2014 categorical-only (square)\n\u2502       \u2514\u2500\u2500 CategoricalEBMFactor \u2014 categorical-only (general)\n\u2514\u2500\u2500 EBMFactor \u2014 factors with energy functions\n</code></pre>"},{"location":"architecture/#conditional-samplers","title":"Conditional samplers","text":"<pre><code>AbstractConditionalSampler\n\u2514\u2500\u2500 AbstractParametricConditionalSampler\n    \u251c\u2500\u2500 BernoulliConditional \u2014 spin-valued Bernoulli sampling\n    \u2502   \u2514\u2500\u2500 SpinGibbsConditional \u2014 Gibbs updates for spin EBMs\n    \u2514\u2500\u2500 SoftmaxConditional \u2014 categorical softmax sampling\n        \u2514\u2500\u2500 CategoricalGibbsConditional \u2014 Gibbs updates for categorical EBMs\n</code></pre>"},{"location":"architecture/#observers","title":"Observers","text":"<pre><code>AbstractObserver\n\u251c\u2500\u2500 StateObserver \u2014 records raw states at each observation step\n\u2514\u2500\u2500 MomentAccumulatorObserver \u2014 online moment accumulation (mean, variance, etc.)\n</code></pre>"},{"location":"architecture/#ebms","title":"EBMs","text":"<pre><code>AbstractEBM\n\u2514\u2500\u2500 AbstractFactorizedEBM \u2014 energy = sum of factor energies\n    \u2514\u2500\u2500 IsingEBM \u2014 standard Ising model (biases + pairwise couplings)\n</code></pre>"},{"location":"architecture/#programs_1","title":"Programs","text":"<pre><code>BlockSamplingProgram \u2014 core padded block Gibbs engine\n\u2514\u2500\u2500 FactorSamplingProgram \u2014 auto-converts factors \u2192 interaction groups\n    \u2514\u2500\u2500 IsingSamplingProgram \u2014 thin Ising-specific wrapper\n</code></pre>"},{"location":"architecture/#limitations","title":"Limitations","text":"<p>Sampling is a fundamentally hard problem. Generating samples from a high-dimensional distribution can require many steps even with parallelized proposals. THRML is focused on Gibbs sampling; for general sampling it is not always clear when Gibbs is substantially faster or slower than other MCMC methods. As a concrete example: a two-node Ising model with \\(J = -\\infty, h = 0\\) has two ground states \\(\\{-1,-1\\}\\) and \\(\\{1,1\\}\\); Gibbs sampling will never mix between them, while uniform Metropolis\u2013Hastings converges quickly.</p>"},{"location":"api/block_management/","title":"Block Management","text":""},{"location":"api/block_management/#thrml_boost.Block","title":"<code>thrml_boost.Block</code> <code></code>","text":"<p>A Block is the basic unit through which Gibbs sampling can operate.</p> <p>Each block represents a collection of nodes that can efficiently be sampled simultaneously in a JAX-friendly SIMD manner. In THRML, this means that the nodes must all be of the same type.</p> <p>Attributes:</p> <ul> <li><code>nodes</code>: the tuple of nodes that this block contains</li> </ul>"},{"location":"api/block_management/#thrml_boost.BlockSpec","title":"<code>thrml_boost.BlockSpec</code> <code></code>","text":"<p>This contains the necessary mappings for logging indices of states and node types.</p> <p>This helps convert between block states and global states. A block state is a list of pytrees, where each pytree leaf has shape[0] = number of nodes in the block. The length of the block state is the number of blocks. The global state is a flattened version of this. Each pytree type is combined (regardless of which block they are in), to make a list of pytrees where each leaf shape[0] is the total number of nodes of that pytree shape. As an example, imagine an Ising model, every node is the same pytree (just a scalar array), as such the block state is a list of arrays where each array is the state of the block and the global state would be a length-1 list that contains an array of shape (total_nodes,).</p> <p>Why is this global/block representation necessary? The answer is that the global representation is preferred for operating over in many JAX cases, but requires careful indexing (to know where in this long array each block resides) and thus the block representation is more natural/easy to use for many users. Why is the global state easier to work with? Well consider sampling, in order to sample a block (or even just a node) we need to collect all the states of the neighboring nodes. If we only had the block state we would have to loop over the block state and collect from each block the neighbors, we would then pass this to the sampler. The sampler would then have to know the type of each block (to know what to do with the states) then for loop over the blocks in order to collect each. This (programmatically) is fine, but results in additional for loops that slow down JAX, compared to gathering indexes from a single array.</p> <p>Attributes:</p> <ul> <li><code>blocks</code>: the list of blocks this spec contains</li> <li><code>all_block_sds</code>: a SD is a single <code>_PyTreeStruct</code>. Each node/block has only     one SD associated with it, but each node can have neighbors of many types.     This is the SD of each block (in the same order as blocks, this internal     ordering is quite important for bookkeeping). This list is just the list     of SDs for each block (and thus has length = len(blocks)).</li> <li><code>global_sd_order</code>: the list of SDs, providing a SoT for the global ordering</li> <li><code>sd_index_map</code>: a dictionary mapping the SD to an integer in the     <code>global_sd_order</code>. This is like calling <code>.index</code> on it.</li> <li><code>node_global_location_map</code>: a dictionary mapping a given node to a tuple.     That tuple contains the global index (i.e. which element in the global     list it is in) and the relative position in that pytree. That is to say,     you can get the state of the node via     <code>map(x[tuple[1]], global_repr[tuple[0]])</code></li> <li><code>block_to_global_slice_spec</code>: a list over unique SDs (so length     global_sd_order), where each list inside this is the list over blocks     which contain that pytree. E.g. [[0, 1], [2]] indicates that blocks[0]     and blocks[1] are both of pytree SD 0.</li> <li><code>node_shape_dtypes</code>: a dictionary mapping node types to hashable <code>_PyTreeStruct</code></li> <li><code>node_shape_struct</code>: a dictionary mapping node types to pytrees of JAX-shaped     dtype structs (just for user access, since the keys aren't hashable that     creates issues for JAX in other areas.)</li> </ul>"},{"location":"api/block_management/#thrml_boost.BlockSpec.__init__","title":"<code>__init__(blocks: list[thrml_boost.Block], node_shape_dtypes: Mapping[type[thrml_boost.AbstractNode], PyTree[jax.ShapeDtypeStruct]])</code>","text":"<p>Create a BlockSpec from blocks.</p> <p>Based on the information passed in via node_shape_dtypes, determine the minimal global state that can be used to represent the blocks.</p> <p>Arguments:</p> <ul> <li><code>blocks</code>: the list of <code>Block</code>s that this specification operates on</li> <li><code>node_shape_dtypes</code>: the mapping of node types to their structures. This         should be a pytree of <code>jax.ShapeDtypeStruct</code>s.</li> </ul>"},{"location":"api/block_management/#thrml_boost.block_state_to_global","title":"<code>thrml_boost.block_state_to_global(block_state: list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']], spec: thrml_boost.BlockSpec) -&gt; list[PyTree[Shaped[Array, 'nodes_global ?*state'], '_GlobalState']]</code> <code></code>","text":"<p>Convert block-local state to the global stacked representation.</p> <p>The block representation is a list where <code>block_state[i]</code> contains the state of <code>spec.blocks[i]</code> and every node occupies index 0 of its leaf.</p> <p>The global representation is a shorter list (one entry per distinct PyTree structure) in which all blocks with the same structure are concatenated along their node axis.</p> <p>Arguments:</p> <ul> <li><code>block_state</code>: State organised per block, same length as     <code>spec.blocks</code>.</li> <li><code>spec</code>: The [<code>thrml.BlockSpec</code>][] that defines the mapping.</li> </ul> <p>Returns:</p> <p>A list whose length equals <code>len(spec.global_sd_order)</code>\u2014the stacked global state.</p>"},{"location":"api/block_management/#thrml_boost.get_node_locations","title":"<code>thrml_boost.get_node_locations(nodes: thrml_boost.Block, spec: thrml_boost.BlockSpec) -&gt; tuple[int, Int[Array, 'nodes']]</code> <code></code>","text":"<p>Locate a contiguous set of nodes inside the global state.</p> <p>Arguments:</p> <ul> <li><code>nodes</code>: A [<code>thrml.Block</code>][] whose nodes you want locations for.</li> <li><code>spec</code>: The [<code>thrml.BlockSpec</code>][] generated from the same graph.</li> </ul> <p>Returns:</p> <p>Tuple <code>(sd_index, positions)</code> where</p> <ul> <li>sd_index is the position inside the global list returned by   [<code>thrml.block_state_to_global</code>][], and</li> <li>positions is a 1D array with the indices each node   occupies inside that particular PyTree.</li> </ul>"},{"location":"api/block_management/#thrml_boost.from_global_state","title":"<code>thrml_boost.from_global_state(global_state: list[PyTree[Shaped[Array, 'nodes_global ?*state'], '_GlobalState']], spec_from: thrml_boost.BlockSpec, blocks_to_extract: list[thrml_boost.Block]) -&gt; list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']]</code> <code></code>","text":"<p>Extract the states for a subset of blocks from a global state.</p> <p>Arguments:</p> <ul> <li><code>global_state</code>: A state produced by     [<code>thrml.block_state_to_global(spec_from)</code>][].</li> <li><code>spec_from</code>: The [<code>thrml.BlockSpec</code>][] associated with global_state.</li> <li><code>blocks_to_extract</code>: The blocks whose node states should be returned.</li> </ul> <p>Returns:</p> <p>A list with one element per blocks_to_extract\u2014each element is a PyTree with exactly <code>len(block)</code> nodes in its leading dimension.</p>"},{"location":"api/block_management/#thrml_boost.make_empty_block_state","title":"<code>thrml_boost.make_empty_block_state(blocks: list[thrml_boost.Block], node_shape_dtypes: Mapping[type[thrml_boost.AbstractNode], PyTree[jax.ShapeDtypeStruct]], batch_shape: tuple | None = None) -&gt; list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']]</code> <code></code>","text":"<p>Allocate a zero-initialised block state.</p> <p>Arguments:</p> <ul> <li><code>blocks</code>: All blocks in the graph (order is preserved).</li> <li><code>node_shape_dtypes</code>: Maps every node class to its     <code>jax.ShapeDtypeStruct</code> PyTree template.</li> <li><code>batch_shape</code>: Optional batch dimension(s) to prepend to every leaf.</li> </ul> <p>Returns:</p> <p>A list of PyTrees\u2014one per block\u2014whose leaves are <code>zeros(batch_shape + (len(block),) + leaf.shape)</code>.</p>"},{"location":"api/block_management/#thrml_boost.verify_block_state","title":"<code>thrml_boost.verify_block_state(blocks: list[thrml_boost.Block], states: list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']], node_shape_dtypes: Mapping[type[thrml_boost.AbstractNode], PyTree[jax.ShapeDtypeStruct]], block_axis: int | None = None) -&gt; None</code> <code></code>","text":"<p>Check that a state is what it should be given some blocks and node shape/dtypes.</p> <p>Passing incompatible state information into THRML functions can lead to unintended casting/other weird silent errors, so we should always check this.</p> <p>Arguments:</p> <ul> <li><code>blocks</code>: A list of Blocks.</li> <li><code>states</code>: A list of states to verify against blocks.</li> <li><code>node_shape_dtypes</code>: Maps every node class to its     <code>jax.ShapeDtypeStruct</code> PyTree template.</li> <li><code>block_axis</code>: Index in the state batch shape at which to expect the block length.</li> </ul> <p>Returns:</p> <p>None. Raises RuntimeError if blocks and states are incompatible.</p>"},{"location":"api/block_sampling/","title":"Block Sampling","text":""},{"location":"api/block_sampling/#thrml_boost.BlockGibbsSpec","title":"<code>thrml_boost.BlockGibbsSpec(thrml_boost.BlockSpec)</code> <code></code>","text":"<p>A BlockGibbsSpec is a type of BlockSpec which contains additional information on free and clamped blocks.</p> <p>This entity also supports <code>SuperBlock</code>s, which are merely groups of blocks which are sampled at the same time algorithmically, but not programmatically. That is to say, superblock = (block1, block2) means that the states input to block1 and block2 are the same, but they are not executed at the same time. This may be because they are the same color on a graph, but require vastly different sampling methods such that JAX SIMD approaches are not feasible to parallelize them.</p> <p>A recurring theme in <code>thrml</code> is the importance of implicit indexing. One such example can be seen here. Because global states are created by concatenating lists of free and clamped blocks, providing the inputs in the same order as the blocks are defined is essential. This is almost always taken care of internally, but when writing custom functions or interfaces this is important to keep in mind.</p> <p>Attributes:</p> <ul> <li><code>free_blocks</code>: the list of free blocks (in order)</li> <li><code>sampling_order</code>: a list of <code>len(superblocks)</code> lists, where each     <code>sampling_order[i]</code> is the index of <code>free_blocks</code> to sample.     Sampling is done by iterating over this order and sampling each     sublist of free blocks at the same algorithmic time.</li> <li><code>clamped_blocks</code>: the list of clamped blocks</li> <li><code>superblocks</code>: the list of superblocks</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.BlockGibbsSpec.__init__","title":"<code>__init__(free_super_blocks: Sequence[tuple[thrml_boost.Block, ...] | thrml_boost.Block], clamped_blocks: list[thrml_boost.Block], node_shape_dtypes: Mapping[type[thrml_boost.AbstractNode], PyTree[jax.ShapeDtypeStruct]] = {thrml_boost.SpinNode: ShapeDtypeStruct(shape=(), dtype=bool), thrml_boost.CategoricalNode: ShapeDtypeStruct(shape=(), dtype=uint8)})</code>","text":"<p>Create a Gibbs specification from free and clamped blocks.</p> <p>Arguments:</p> <ul> <li><code>free_super_blocks</code>: An ordered sequence where each element is either     a single <code>Block</code>, or a tuple of blocks that must share the same global     state when calling their individual samplers.</li> <li><code>clamped_blocks</code>: Blocks whose nodes stay fixed during sampling.</li> <li><code>node_shape_dtypes</code>: Mapping from node class to a PyTree of     <code>jax.ShapeDtypeStruct</code>; identical to the argument in <code>BlockSpec</code>.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.BlockSamplingProgram","title":"<code>thrml_boost.BlockSamplingProgram</code> <code></code>","text":"<p>A PGM block-sampling program.</p> <p>This class encapsulates everything that is needed to run a PGM block sampling program in THRML. <code>per_block_interactions</code> and <code>per_block_interaction_active</code> are parallel to the free blocks in <code>gibbs_spec</code>, and their members are passed directly to a sampler when the state of the corresponding free block is being updated during a sampling program. <code>per_block_interaction_global_inds</code> and <code>per_block_interaction_global_slices</code> are also parallel to the free blocks, and are used to slice the global state of the program to produce the state information required to update the state of each block alongside the static information contained in the interactions.</p> <p>Attributes:</p> <ul> <li><code>gibbs_spec</code>: A division of some PGM into free and clamped blocks.</li> <li><code>samplers</code>: A sampler to use to update every free block in <code>gibbs_spec</code>.</li> <li><code>per_block_interactions</code>: All the interactions that touch each free block in <code>gibbs_spec</code>.</li> <li><code>per_block_interaction_active</code>: indicates which interactions are real     and which interactions are not part of the model and have been added to pad data structures so that they     can be rectangular.</li> <li><code>per_block_interaction_global_inds</code>: how to find the information required to update each block within the global     state list</li> <li><code>per_block_interaction_global_slices</code>: how to slice each array in the global state list to find the information     required to update each block</li> <li><code>_block_sd_inds</code>: precomputed sd_index for each free block (avoids recomputing inside scan)</li> <li><code>_block_positions</code>: precomputed node positions in global state for each free block (avoids recomputing inside scan)</li> <li><code>_block_output_sds</code>: precomputed output ShapeDtypeStruct pytree for each free block</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.BlockSamplingProgram.__init__","title":"<code>__init__(gibbs_spec: thrml_boost.BlockGibbsSpec, samplers: list[thrml_boost.AbstractConditionalSampler], interaction_groups: list[thrml_boost.InteractionGroup])</code>","text":"<p>Construct a <code>BlockSamplingProgram</code>.</p> <p>This code is the beating heart of THRML, and the chance that you should be modifying it or trying to understand it deeply are very low (as this would basically correspond to re-writing the library). This code takes in a set of information that implicitly defines a sampling program and manipulates it into a shape that is appropriate for practical vectorized block-sampling program. This involves reindexing, slicing, and often padding.</p> <p>Arguments:</p> <ul> <li><code>gibbs_spec</code>: A division of some PGM into free and clamped blocks.</li> <li><code>samplers</code>: The update rule to use for each free block in <code>gibbs_spec</code>.</li> <li><code>interaction_groups</code>: A list of <code>InteractionGroups</code> that define how the     variables in your sampling program affect one another.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.SamplingSchedule","title":"<code>thrml_boost.SamplingSchedule</code> <code></code>","text":"<p>Represents a sampling schedule for a process.</p> <p>Attributes:</p> <ul> <li><code>n_warmup</code>: The number of warmup steps to run before collecting samples.</li> <li><code>n_samples</code>: The number of samples to collect.</li> <li><code>steps_per_sample</code>: The number of steps to run between each sample.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.sample_blocks","title":"<code>thrml_boost.sample_blocks(key: Key[Array, ''], state_free: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], clamp_state: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], program: thrml_boost.BlockSamplingProgram, sampler_state: list[~_SamplerState]) -&gt; tuple[list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], list[~_SamplerState]]</code> <code></code>","text":"<p>Perform one iteration of sampling, visiting every block.</p> <p>Arguments:</p> <ul> <li><code>key</code>: The JAX PRNG key.</li> <li><code>state_free</code>: The state of the free blocks.</li> <li><code>clamp_state</code>: The state of the clamped blocks.</li> <li><code>program</code>: The Gibbs program.</li> <li><code>sampler_state</code>: The state of the sampler.</li> </ul> <p>Returns:</p> <ul> <li>Updated free-block state list and sampler-state list.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.sample_single_block","title":"<code>thrml_boost.sample_single_block(key: Key[Array, ''], state_free: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], clamp_state: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], program: thrml_boost.BlockSamplingProgram, block: int, sampler_state: ~_SamplerState, global_state: list[PyTree] | None = None, per_block_interactions: list[list[PyTree]] | None = None) -&gt; tuple[PyTree[Shaped[Array, 'nodes ?*state'], '_State'], ~_SamplerState]</code> <code></code>","text":"<p>Samples a single block within a Gibbs sampling program based on the current states and program configurations. It extracts neighboring states, processes required data, and applies a sampling function to generate output samples.</p> <p>Arguments:</p> <ul> <li><code>key</code>: Pseudo-random number generator key to ensure reproducibility of sampling.</li> <li><code>state_free</code>: Current states of free blocks, representing the values to be     updated during sampling.</li> <li><code>clamp_state</code>: Clamped states that remain fixed during the sampling process.</li> <li><code>program</code>: The Gibbs sampling program containing specifications, samplers,     neighborhood information, and parameters.</li> <li><code>block</code>: Index of the block to be sampled in the current iteration.</li> <li><code>sampler_state</code>: The current state of the sampler that will be used to     perform the update.</li> <li><code>global_state</code>: Optionally precomputed global state for the concatenated     free and clamped blocks; when omitted the function constructs it internally.</li> <li><code>per_block_interactions</code>: Optional override for the interaction weights. When     provided (e.g. inside a vmapped multi-chain runner), this is used instead of     <code>program.per_block_interactions</code>. The caller is responsible for ensuring the     PyTree structure matches <code>program.per_block_interactions</code>.</li> </ul> <p>Returns:</p> <ul> <li>Updated block state and sampler state for the specified block.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.sample_with_observation","title":"<code>thrml_boost.sample_with_observation(key: Key[Array, ''], program: thrml_boost.BlockSamplingProgram, schedule: thrml_boost.SamplingSchedule, init_chain_state: list[PyTree[Shaped[Array, 'nodes ?*state']]], state_clamp: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], observation_carry_init: ~ObserveCarry, f_observe: thrml_boost.AbstractObserver) -&gt; tuple[~ObserveCarry, list[PyTree[Shaped[Array, 'n_samples nodes ?*state']]]]</code> <code></code>","text":"<p>Run the full chain and call an Observer after every recorded sample.</p> <p>Arguments:</p> <ul> <li><code>key</code>: RNG key.</li> <li><code>program</code>: The sampling program.</li> <li><code>schedule</code>: Warm-up length, number of samples, number of steps between samples.</li> <li><code>init_chain_state</code>: Initial free-block state.</li> <li><code>state_clamp</code>: Clamped-block state.</li> <li><code>observation_carry_init</code>: Initial carry handed to <code>f_observe</code>.</li> <li><code>f_observe</code>: Observer instance.</li> </ul> <p>Returns:</p> <ul> <li>Tuple <code>(final_observer_carry, samples)</code> where <code>samples</code> is a PyTree whose     leading axis has size <code>schedule.n_samples</code>.</li> </ul>"},{"location":"api/block_sampling/#thrml_boost.sample_states","title":"<code>thrml_boost.sample_states(key: Key[Array, ''], program: thrml_boost.BlockSamplingProgram, schedule: thrml_boost.SamplingSchedule, init_state_free: list[PyTree[Shaped[Array, 'nodes ?*state']]], state_clamp: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], nodes_to_sample: list[thrml_boost.Block]) -&gt; list[PyTree[Shaped[Array, 'n_samples nodes ?*state']]]</code> <code></code>","text":"<p>Convenience wrapper to collect state information for nodes_to_sample only.</p> <p>Internally builds a [<code>thrml.StateObserver</code>][], runs [<code>thrml.sample_with_observation</code>][], and returns a stacked tensor of shape <code>(schedule.n_samples, ...)</code>.</p>"},{"location":"api/conditional_samplers/","title":"Conditional Samplers","text":""},{"location":"api/conditional_samplers/#thrml_boost.AbstractConditionalSampler","title":"<code>thrml_boost.AbstractConditionalSampler</code> <code></code>","text":"<p>Base class for all conditional samplers.</p> <p>A conditional sampler is used to update the state of a block of nodes during each iteration of a sampling algorithm. It takes in the states of all the neighbors and produces a sample for the current block of nodes. This can often be done exactly, but need not be. One could embed MCMC methods within this sampler (to do Metropolis within Gibbs, for example).</p>"},{"location":"api/conditional_samplers/#thrml_boost.AbstractConditionalSampler.sample","title":"<code>sample(key: Key, interactions: list[PyTree], active_flags: list[Array], states: list[list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']]], sampler_state: ~_SamplerState, output_sd: PyTree[jax.ShapeDtypeStruct]) -&gt; tuple[PyTree[Shaped[Array, 'nodes ?*state'], 'State'], ~_SamplerState]</code>","text":"<p>Draw a sample from this conditional.</p> <p>If this sampler is involved in a block sampling program, this function is called every iteration to update the state of a block of nodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: A RNG key that the sampler can use to sample from distributions using <code>jax.random</code>.</li> <li><code>interactions</code>: A list of interactions that influence the result of this block update. Each interaction     is a PyTree. Each array in the PyTree will have shape [n, k, ...], where n is the number of nodes in      the block that is being updated and k is the maximum number of times any node in this block was      detected as a head node for this interaction.</li> <li><code>active_flags</code>: A list of arrays of flags that is parallel to interactions. Each array indicates which     instances of a given interaction are active for each node in the block. This array has shape [n, k],     and is False if a given instance is inactive (which means that it should be ignored during the      computation that happens in this function).</li> <li><code>states</code>: A list of PyTrees that is parallel to interactions, representing the sampling state information     that is relevant to computing the influence of each interaction. Every array in each PyTree will have     shape [n, k, ...].</li> <li><code>sampler_state</code>: The current state of this sampler. Will be replaced by the second return from this function     the next time it is called.</li> <li><code>output_sd</code>: A PyTree indicating the expected shape/dtype of the output of this function.</li> </ul> <p>Returns:</p> <p>A new state for the block of nodes, matching the template given by <code>output_sd</code>.</p>"},{"location":"api/conditional_samplers/#thrml_boost.AbstractParametricConditionalSampler","title":"<code>thrml_boost.AbstractParametricConditionalSampler(thrml_boost.AbstractConditionalSampler)</code> <code></code>","text":"<p>A conditional sampler that leverages a parameterized distribution.</p> <p>When <code>sample</code> is called, this sampler will first compute a set of parameters, and then use those parameters to draw a sample from some distribution. This workflow is frequently useful in practical cases; for example, to sample from a Gaussian, we can first compute a mean vector and covariance matrix using any procedure, and then draw a sample from the corresponding Gaussian distribution by appropriately transforming a vector of standard normal random variables.</p>"},{"location":"api/conditional_samplers/#thrml_boost.AbstractParametricConditionalSampler.compute_parameters","title":"<code>compute_parameters(key: Key, interactions: list[PyTree], active_flags: list[Array], states: list[list[PyTree[Shaped[Array, 'nodes ?*state'], 'State']]], sampler_state: PyTree, output_sd: PyTree[jax.ShapeDtypeStruct]) -&gt; PyTree</code>","text":"<p>Compute the parameters of the distribution. For a description of the arguments, see [<code>thrml.AbstractConditionalSampler.sample</code>][]</p>"},{"location":"api/conditional_samplers/#thrml_boost.BernoulliConditional","title":"<code>thrml_boost.BernoulliConditional(thrml_boost.AbstractParametricConditionalSampler)</code> <code></code>","text":"<p>Sample from a bernoulli distribution.</p> <p>This sampler is designed to sample from a spin-valued bernoulli distribution:</p> \\[\\mathbb{P}(S=s) \\propto e^{\\gamma s}\\] <p>where \\(S\\) is a spin-valued random variable, \\(s \\in \\{-1, 1\\}\\). The parameter \\(\\gamma\\) must be computed by <code>compute_parameters</code>.</p>"},{"location":"api/conditional_samplers/#thrml_boost.SoftmaxConditional","title":"<code>thrml_boost.SoftmaxConditional(thrml_boost.AbstractParametricConditionalSampler)</code> <code></code>","text":"<p>Sample from a softmax distribution.</p> <p>This sampler samples from the standard softmax distribution:</p> \\[\\mathbb{P}(X=k) \\propto e^{\\theta_k}\\] <p>where \\(X\\) is a categorical random variable and \\(\\theta\\) is a vector that parameterizes the relative probabilities of each of the categories.</p>"},{"location":"api/factor/","title":"Factors","text":""},{"location":"api/factor/#thrml_boost.AbstractFactor","title":"<code>thrml_boost.AbstractFactor</code> <code></code>","text":"<p>A factor represents a batch of undirected interactions between sets of random variables.</p> <p>Concretely, this class implements a batch of factors defined over a bunch of parallel node groups. A single factor is defined over the nodes given by node_groups[k][i] for all k and a particular i. The defining trait of a factor is to produce InteractionGroups that affect each member of the factor in some way during the conditional updates of a block sampling program. As a user, you specify how this is done by implementing a concrete to_interaction_groups method for your child class.</p> <p>Attributes:</p> <ul> <li><code>node_groups</code>: the list of blocks that makes up this batch of factors.</li> </ul>"},{"location":"api/factor/#thrml_boost.AbstractFactor.to_interaction_groups","title":"<code>to_interaction_groups() -&gt; list[thrml_boost.InteractionGroup]</code>","text":"<p>Compile a factor to a set of directed interactions.</p>"},{"location":"api/factor/#thrml_boost.WeightedFactor","title":"<code>thrml_boost.WeightedFactor(thrml_boost.AbstractFactor)</code> <code></code>","text":"<p>A factor that is parameterized by a weight tensor.</p> <p>The leading dimension of the weights tensor must be the same length as the batch dimension of the factor (i.e the number of nodes in each of the node_groups).</p> <p>Attributes:</p> <ul> <li><code>weights</code>: the weight tensor.</li> </ul>"},{"location":"api/factor/#thrml_boost.FactorSamplingProgram","title":"<code>thrml_boost.FactorSamplingProgram(thrml_boost.BlockSamplingProgram)</code> <code></code>","text":"<p>A sampling program built out of factors.</p> <p>This class simply breaks each factor passed to it down into interaction groups and uses them to build a BlockSamplingProgram.</p>"},{"location":"api/factor/#thrml_boost.FactorSamplingProgram.__init__","title":"<code>__init__(gibbs_spec: thrml_boost.BlockGibbsSpec, samplers: list[thrml_boost.AbstractConditionalSampler], factors: Sequence[thrml_boost.AbstractFactor], other_interaction_groups: list[thrml_boost.InteractionGroup])</code>","text":"<p>Create a FactorSamplingProgram. Thin wrapper over <code>BlockSamplingProgram</code>.</p> <p>Arguments:</p> <ul> <li><code>gibbs_spec</code>: A division of some PGM into free and clamped blocks.</li> <li><code>samplers</code>: The update rule to use for each free block in gibbs_spec.</li> <li><code>factors</code>: The factors to use to build this sampling program.</li> <li><code>other_interaction_groups</code>: Other interaction groups to include in your program alongside what the     factors produce.</li> </ul>"},{"location":"api/interaction/","title":"Interaction Groups","text":""},{"location":"api/interaction/#thrml_boost.InteractionGroup","title":"<code>thrml_boost.InteractionGroup</code> <code></code>","text":"<p>Defines computational dependencies for conditional sampling updates.</p> <p>An <code>InteractionGroup</code> specifies information that is required to update the state of some subset of the nodes of a PGM during a block sampling routine.</p> <p>More concretely, when the state of the node at head_nodes[i] is being updated, the sampler will receive the current state of the nodes at tail_nodes[k][i] for all k, and the ith element of each array in the Interaction PyTree (sliced along the first dimension).</p> <p>Attributes:</p> <ul> <li><code>head_nodes</code>: these are the nodes whose conditional updates should be affected by this InteractionGroup.</li> <li><code>tail_nodes</code>: these are the nodes whose state information is required to update <code>head_nodes</code>.</li> <li><code>interaction</code>: this specifies the parametric (independent of the state of the sampling program)     required to update 'head_nodes'.</li> </ul>"},{"location":"api/interaction/#thrml_boost.InteractionGroup.__init__","title":"<code>__init__(interaction: PyTree, head_nodes: thrml_boost.Block, tail_nodes: list[thrml_boost.Block])</code>","text":"<p>Create an <code>InteractionGroup</code>.</p> <p>An <code>InteractionGroup</code> implements a group of directed interactions between nodes in a PGM sampling program.</p> <p>Arguments:</p> <ul> <li><code>interaction</code>: A PyTree specifying the static information associated with the     interaction. The first dimension of every Array in interaction must be equal     to the length of <code>head_nodes</code>.</li> <li><code>head_nodes</code>: The nodes whose update is affected by the interaction.</li> <li><code>tail_nodes</code>: The groups of nodes whose state is required to update     <code>head_nodes</code>. Each block in this list of blocks is intended to be parallel     to <code>head_nodes</code>. i.e, to update the state of head_nodes[i] during sampling     we need state info about tail_nodes[k][i] for all values of k.</li> </ul>"},{"location":"api/observers/","title":"Sampling Observers","text":""},{"location":"api/observers/#thrml_boost.AbstractObserver","title":"<code>thrml_boost.AbstractObserver</code> <code></code>","text":"<p>Interface for objects that inspect the sampling program while it is running.</p> <p>A concrete Observer is called once per block-sampling iteration and can maintain an arbitrary \"carry\" state across calls (e.g. running averages, histogram buffers, log-probs, etc.).</p>"},{"location":"api/observers/#thrml_boost.AbstractObserver.init","title":"<code>init() -&gt; PyTree</code>","text":"<p>Initialize the memory for the observer. Defaults to None.</p>"},{"location":"api/observers/#thrml_boost.StateObserver","title":"<code>thrml_boost.StateObserver(thrml_boost.AbstractObserver)</code> <code></code>","text":"<p>Observer which logs the raw state of some set of nodes.</p> <p>This observer is stateless: its carry is always <code>None</code> and <code>iteration</code> is ignored.</p> <p>Attributes:</p> <ul> <li><code>blocks_to_sample</code>: the list of <code>Block</code>s which the states are logged for</li> </ul>"},{"location":"api/observers/#thrml_boost.StateObserver.__init__","title":"<code>__init__(blocks_to_sample: list[thrml_boost.Block])</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/observers/#thrml_boost.MomentAccumulatorObserver","title":"<code>thrml_boost.MomentAccumulatorObserver(thrml_boost.AbstractObserver)</code> <code></code>","text":"<p>Observer that accumulates and updates the provided moments.</p> <p>It doesn't log any samples, and will only accumulate moments. Note that this observer does not scale the accumulated values by the number of times it was called. It simply records a running sum of a product of some state variables,</p> \\[\\sum_i f(x_1^i) f(x_2^i) \\dots f(x_N^i)\\] <p>Attributes:</p> <ul> <li><code>blocks_to_sample</code>: the blocks to accumulate the moments over. These     are for constructing the final state, and aren't truly \"blocks\"     in the algorithmic sense (they can be connected to each other).     There is one block per node type.</li> <li><code>flat_nodes_list</code>: a list of all of the nodes in the moments (each     occurring only once, so len(set(x)) = len(x)).</li> <li><code>flat_to_type_slices_list</code>: a list over node types in which each element     is an array of indices of the <code>flat_node_list</code> which that type     corresponds to</li> <li><code>flat_to_full_moment_slices</code>: a list over moment types in which each     element is a 2D array, which matches the shape of the <code>moment_spec[i]</code>     and of which each element is the index in the <code>flat_node_list</code>.</li> <li><code>f_transform</code>: the element-wise transformation \\(f\\) to apply to sample values before     accumulation.</li> <li><code>_flat_scatter_index</code>: precomputed concatenation of all <code>flat_to_type_slices_list</code>     arrays, used to build <code>flat_state</code> in a single scatter call.</li> <li><code>_flat_scatter_sizes</code>: number of entries contributed by each node type, used to     split the concatenated sampled state before scattering.</li> <li><code>_flat_value_order</code>: precomputed <code>argsort(_flat_scatter_index)</code>; used in     <code>__call__</code> to permute the concatenated sampled values into flat-node     order without allocating a zeros array.</li> <li><code>_accumulate_dtype</code>: dtype for the accumulator, fixed at construction time.</li> </ul>"},{"location":"api/observers/#thrml_boost.MomentAccumulatorObserver.__init__","title":"<code>__init__(moment_spec: Sequence[Sequence[Sequence[thrml_boost.AbstractNode]]], f_transform: typing.Callable = &lt;function _f_identity&gt;, dtype: numpy.dtype = jax.numpy.float32)</code>","text":"<p>Create a MomentAccumulatorObserver.</p> <p>Arguments:</p> <ul> <li> <p><code>moment_spec</code>: A 3 depth sequence. The first is a sequence over different moment types.     A given moment type should have the same number of nodes in each moment. Then for each     moment type, there is a sequence over moments. Each given moment is defined by a certain     set of nodes.</p> <p>For example, to get the first and second moments on a simple o-o graph:</p> <p>[     [(node1,), (node2,)],     [(node1, node2)] ]</p> </li> <li> <p><code>f_transform</code>: A function that takes in (state, blocks) and returns something with the same     structure as state. Defines a transformation \\(y=f(x)\\) so accumulated moments are     \\(\\langle f(x_1) f(x_2) \\rangle\\).</p> </li> <li> <p><code>dtype</code>: Accumulator dtype, fixed at construction. Defaults to <code>jnp.float32</code>. Use     <code>jnp.float64</code> for double-precision models. Fixing this here avoids a per-step cast     inside the scan body.</p> </li> </ul>"},{"location":"api/pgm/","title":"Graphical Model Components","text":""},{"location":"api/pgm/#thrml_boost.AbstractNode","title":"<code>thrml_boost.AbstractNode</code> <code></code>","text":"<p>A node in a PGM.</p> <p>Every node used in a PGM must inherit from this class. When compiling a program, each node is assigned a shape and datatype that are used to organize the state of the sampling program in a jax-friendly way.</p>"},{"location":"api/pgm/#thrml_boost.SpinNode","title":"<code>thrml_boost.SpinNode(thrml_boost.AbstractNode)</code> <code></code>","text":"<p>A node that represents a random variable that takes on a state in {-1, 1}.</p>"},{"location":"api/pgm/#thrml_boost.CategoricalNode","title":"<code>thrml_boost.CategoricalNode(thrml_boost.AbstractNode)</code> <code></code>","text":"<p>A node that represents a random variable that may take on any one of K possible discrete states, represented by a positive integer in (0, K].</p>"},{"location":"api/tempering/","title":"Parallel Tempering","text":""},{"location":"api/tempering/#thrml_boost.parallel_tempering","title":"<code>thrml_boost.parallel_tempering(key, ebms: Sequence[thrml_boost.models.AbstractEBM], programs: Sequence[thrml_boost.BlockSamplingProgram], init_states: Sequence[list], clamp_state: list, n_rounds: int, gibbs_steps_per_round: int, sampler_states: Sequence[list] | None = None)</code> <code></code>","text":"<p>Run parallel tempering across a sequence of tempered chains.</p> <p>Each round performs block Gibbs updates in every chain in parallel via <code>jax.vmap</code>, then proposes swaps between adjacent temperatures using alternating even/odd pair selection.</p> <p>All chains must share the same block structure (free + clamped) and clamped state. Only the EBM weights (encoded in <code>per_block_interactions</code>) may differ across chains. This is the standard parallel tempering setup where chains differ only by their inverse temperature beta.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>ebms</code>: One EBM per temperature, ordered from lowest to highest beta.</li> <li><code>programs</code>: One <code>BlockSamplingProgram</code> per temperature. All must share     the same block structure; only <code>per_block_interactions</code> may differ.</li> <li><code>init_states</code>: Initial free-block states, one list per chain.</li> <li><code>clamp_state</code>: Clamped-block state, shared across all chains.</li> <li><code>n_rounds</code>: Number of Gibbs + swap rounds to run.</li> <li><code>gibbs_steps_per_round</code>: Gibbs sweeps per chain per round.</li> <li><code>sampler_states</code>: Optional initial sampler states; inferred from programs     if not provided.</li> </ul> <p>Returns:</p> <p>Tuple <code>(states, sampler_states, stats)</code> where <code>stats</code> is a dict with keys <code>accepted</code>, <code>attempted</code>, and <code>acceptance_rate</code>, each an array of length <code>n_chains - 1</code> indexed by adjacent pair.</p>"},{"location":"api/models/discrete_ebm/","title":"Discrete Energy-Based Models","text":"<p>This module contains implementations of discrete energy-based models.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.DiscreteEBMFactor","title":"<code>thrml_boost.models.DiscreteEBMFactor(thrml_boost.models.EBMFactor, thrml_boost.WeightedFactor)</code> <code></code>","text":"<p>Implements batches of energy function terms of the form s_1 * ... * s_M * W[c_1, ..., c_N], where the s_i are spin variables and the c_i are categorical variables.</p> <p>No variable should show up twice in any given interaction. If this happens, the result of sampling from a model that includes the bad factor might not agree with the Boltzmann distribution. For example, the interaction w * s_1 * s_1 * s_2 would violate this rule because s_1 shows up twice. To allow you to do something weird if you want to, this condition has not been enforced in the code.</p> <p>Attributes:</p> <ul> <li><code>spin_node_groups</code>: the node groups involved in the batch of factors that represent spin-valued random variables.</li> <li><code>categorical_node_groups</code>: the node groups involved in the batch of factors that represent categorical-valued     random variables.</li> <li><code>weights</code>: the batch of weight tensors W associated with the factors we are implementing. <code>weights</code> should have     leading dimension b, where b is number of nodes in each element of <code>spin_node_groups</code> and     <code>categorical_node_groups</code>. This tensor has shape [b, x_1, ..., x_N] where b is the number of nodes     in each block and N is the length of <code>categorical_node_groups</code>.</li> <li><code>is_spin</code>: a map that indicates if a given node type represents a spin-valued random variable or not.</li> </ul>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.DiscreteEBMFactor.energy","title":"<code>energy(global_state: list[Array], block_spec: thrml_boost.BlockSpec)</code>","text":"<p>Compute the energy associated with this factor.</p> <p>In this case, that is the sum of terms like s_1 * ... * s_M * W[c_1, ..., c_N].</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.DiscreteEBMFactor.to_interaction_groups","title":"<code>to_interaction_groups() -&gt; list[thrml_boost.InteractionGroup]</code>","text":"<p>Produce interaction groups that implement this factor.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.DiscreteEBMInteraction","title":"<code>thrml_boost.models.DiscreteEBMInteraction</code> <code></code>","text":"<p>An interaction that shows up when sampling from discrete-variable EBMs.</p> <p>Attributes:</p> <ul> <li><code>n_spin</code>: the number of spin states involved in the interaction.</li> <li><code>weights</code>: the weight tensor associated with this interaction.</li> </ul>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.SquareDiscreteEBMFactor","title":"<code>thrml_boost.models.SquareDiscreteEBMFactor(thrml_boost.models.DiscreteEBMFactor)</code> <code></code>","text":"<p>A discrete factor with a square interaction weight tensor (shape [b, x, x, ..., x]).</p> <p>If a discrete factor is square, the interaction groups corresponding to different choices of the head node blocks can be merged. This could yield smaller XLA programs and improved runtime performance via more efficient use of accelerators.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.SquareDiscreteEBMFactor.to_interaction_groups","title":"<code>to_interaction_groups() -&gt; list[thrml_boost.InteractionGroup]</code>","text":""},{"location":"api/models/discrete_ebm/#thrml_boost.models.SpinEBMFactor","title":"<code>thrml_boost.models.SpinEBMFactor(thrml_boost.models.SquareDiscreteEBMFactor)</code> <code></code>","text":"<p>A <code>DiscreteEBMFactor</code> that involves only spin variables.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.CategoricalEBMFactor","title":"<code>thrml_boost.models.CategoricalEBMFactor(thrml_boost.models.DiscreteEBMFactor)</code> <code></code>","text":"<p>A <code>DiscreteEBMFactor</code> that involves only categorical variables.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.SquareCategoricalEBMFactor","title":"<code>thrml_boost.models.SquareCategoricalEBMFactor(thrml_boost.models.SquareDiscreteEBMFactor)</code> <code></code>","text":"<p>A <code>DiscreteEBMFactor</code> that involves only categorical variables that also has a square weight tensor.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.SpinGibbsConditional","title":"<code>thrml_boost.models.SpinGibbsConditional(thrml_boost.BernoulliConditional)</code> <code></code>","text":"<p>A conditional update for spin-valued random variables that will perform a Gibbs sampling update given one or more <code>DiscreteEBMInteractions</code>.</p> <p>This function can be extended to handle a broader class of interactions via inheritance. Specifically, a child class can override the <code>compute_parameters</code> method defined here, compute contributions to \\(\\gamma\\) from other types of interactions, and then call this method to take into account the contributions from <code>DiscreteEBMInteractions</code>.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.SpinGibbsConditional.compute_parameters","title":"<code>compute_parameters(key: Key, interactions: list[PyTree], active_flags: list[Array], states: list[list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']]], sampler_state: None, output_sd: PyTree[jax.ShapeDtypeStruct]) -&gt; PyTree</code>","text":"<p>Compute the parameter \\(\\gamma\\) of a spin-valued Bernoulli distribution given DiscreteEBMInteractions:</p> \\[\\gamma = \\sum_i s_1^i \\dots s_K^i \\: W^i[x_1^i, \\dots, x_M^i]\\] <p>where the sum over \\(i\\) is over all the <code>DiscreteEBMInteractions</code> seen by this function.</p>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.CategoricalGibbsConditional","title":"<code>thrml_boost.models.CategoricalGibbsConditional(thrml_boost.SoftmaxConditional)</code> <code></code>","text":"<p>A conditional update for categorical random variables that will perform a Gibbs sampling update given one or     more <code>DiscreteEBMInteractions</code>.</p> <p>This function can be extended to handle other interactions in the same way as [<code>thrml.models.SpinGibbsConditional</code>][].</p> <p>Attributes:</p> <ul> <li><code>n_categories</code>: how many categories are involved in the softmax distribution this sampler will sample from.</li> </ul>"},{"location":"api/models/discrete_ebm/#thrml_boost.models.CategoricalGibbsConditional.compute_parameters","title":"<code>compute_parameters(key: Key, interactions: list[PyTree], active_flags: list[Array], states: list[list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']]], sampler_state: None, output_sd: PyTree[jax.ShapeDtypeStruct]) -&gt; PyTree</code>","text":"<p>Compute the parameter \\(\\theta\\) of a softmax distribution given DiscreteEBMInteractions:</p> \\[\\theta = \\sum_i s_1^i \\dots s_K^i \\: W^i[:, x_1^i, \\dots, x_M^i]\\] <p>where the sum over \\(i\\) is over all the <code>DiscreteEBMInteractions</code> seen by this function.</p>"},{"location":"api/models/ebm/","title":"Energy-Based Models","text":"<p>This module contains implementations of energy-based models.</p>"},{"location":"api/models/ebm/#thrml_boost.models.AbstractEBM","title":"<code>thrml_boost.models.AbstractEBM</code> <code></code>","text":"<p>Something that has a well-defined energy function (map from a state to a scalar).</p>"},{"location":"api/models/ebm/#thrml_boost.models.AbstractEBM.energy","title":"<code>energy(state: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], blocks: 'BlockSpec | list[Block]') -&gt; Float[Array, '']</code>","text":"<p>Evaluate the energy function of the EBM given some state information.</p> <p>Arguments:</p> <ul> <li><code>state</code>: The state for which to evaluate the energy function. Must be compatible with <code>blocks</code>.</li> <li><code>blocks</code>: Specifies how the information in <code>state</code> is organized. May be either a pre-built     <code>BlockSpec</code> (fast path \u2014 avoids rebuilding the spec) or a plain <code>list[Block]</code> for     convenience when calling from user code.</li> </ul> <p>Returns:</p> <p>A scalar representing the energy value associated with <code>state</code>.</p>"},{"location":"api/models/ebm/#thrml_boost.models.AbstractFactorizedEBM","title":"<code>thrml_boost.models.AbstractFactorizedEBM(thrml_boost.models.AbstractEBM)</code> <code></code>","text":"<p>An EBM that is made up of Factors, i.e., an EBM with an energy function like,</p> \\[\\mathcal{E}(x) = \\sum_i \\mathcal{E}^i(x)\\] <p>where the sum over \\(i\\) is taken over factors.</p> <p>Child classes must define a property which returns a list of factors that substantiate the EBM.</p> <p>Attributes:</p> <ul> <li><code>node_shape_dtypes</code>: the shape/dtypes of the nodes involved in this EBM. Used to generate the BlockSpec that     defines the global state that factors receive to compute energy.</li> </ul>"},{"location":"api/models/ebm/#thrml_boost.models.FactorizedEBM","title":"<code>thrml_boost.models.FactorizedEBM(thrml_boost.models.AbstractFactorizedEBM)</code> <code></code>","text":"<p>An EBM that is defined by a concrete list of factors.</p> <p>Attributes:</p> <ul> <li><code>_factors</code>: the list of factors that defines this EBM.</li> </ul>"},{"location":"api/models/ebm/#thrml_boost.models.FactorizedEBM.__init__","title":"<code>__init__(factors: list[thrml_boost.models.EBMFactor], node_shape_dtypes: Mapping[type[thrml_boost.AbstractNode], PyTree[jax.ShapeDtypeStruct]] = {thrml_boost.SpinNode: ShapeDtypeStruct(shape=(), dtype=bool), thrml_boost.CategoricalNode: ShapeDtypeStruct(shape=(), dtype=uint8)})</code>","text":""},{"location":"api/models/ebm/#thrml_boost.models.FactorizedEBM.energy","title":"<code>energy(state: list[PyTree[Shaped[Array, 'nodes ?*state'], '_State']], blocks: 'BlockSpec | list[Block]') -&gt; Float[Array, '']</code>","text":"<p>Evaluate the total energy as the sum of all factor energies.</p>"},{"location":"api/models/ebm/#thrml_boost.models.EBMFactor","title":"<code>thrml_boost.models.EBMFactor(thrml_boost.AbstractFactor)</code> <code></code>","text":"<p>A factor that defines an energy function.</p>"},{"location":"api/models/ebm/#thrml_boost.models.EBMFactor.energy","title":"<code>energy(global_state: list[Array], block_spec: thrml_boost.BlockSpec) -&gt; Float[Array, '']</code>","text":"<p>Evaluate the energy function of the factor.</p> <p>Arguments:</p> <ul> <li><code>global_state</code>: The state information to use to evaluate the energy function.     Is a global state of <code>block_spec</code>.</li> <li><code>block_spec</code>: The <code>BlockSpec</code> used to generate <code>global_state</code>.</li> </ul>"},{"location":"api/models/ising/","title":"Ising Models","text":"<p>This module contains implementations of Ising models and spin systems.</p>"},{"location":"api/models/ising/#thrml_boost.models.IsingEBM","title":"<code>thrml_boost.models.IsingEBM(thrml_boost.models.AbstractFactorizedEBM)</code> <code></code>","text":"<p>An EBM with the energy function,</p> \\[\\mathcal{E}(s) = -\\beta \\left( \\sum_{i \\in S_1} b_i s_i + \\sum_{(i, j) \\in S_2} J_{ij} s_i s_j \\right)\\] <p>where \\(S_1\\) and \\(S_2\\) are the sets of biases and weights that make up the model, respectively. \\(b_i\\) represents the bias associated with the spin \\(s_i\\) and \\(J_{ij}\\) is a weight that couples \\(s_i\\) and \\(s_j\\). \\(\\beta\\) is the usual temperature parameter.</p> <p>Attributes:</p> <ul> <li><code>nodes</code>: the nodes that have an associated bias (i.e \\(S_1\\))</li> <li><code>biases</code>: the bias associated with each node in <code>nodes</code>.</li> <li><code>edges</code>: the edges that have an associated weight (i.e \\(S_2\\))</li> <li><code>weights</code>: the weight associated with each pair of nodes in <code>edges</code>.</li> <li><code>beta</code>: the scalar temperature parameter for the model.</li> </ul>"},{"location":"api/models/ising/#thrml_boost.models.IsingEBM.__init__","title":"<code>__init__(nodes: list[thrml_boost.AbstractNode], edges: list[tuple[thrml_boost.AbstractNode, thrml_boost.AbstractNode]], biases: Array, weights: Array, beta: Array)</code>","text":""},{"location":"api/models/ising/#thrml_boost.models.IsingSamplingProgram","title":"<code>thrml_boost.models.IsingSamplingProgram(thrml_boost.FactorSamplingProgram)</code> <code></code>","text":"<p>A very thin wrapper on FactorSamplingProgram that specializes it to the case of an Ising Model.</p>"},{"location":"api/models/ising/#thrml_boost.models.IsingSamplingProgram.__init__","title":"<code>__init__(ebm: thrml_boost.models.IsingEBM, free_blocks: list[tuple[thrml_boost.Block, ...] | thrml_boost.Block], clamped_blocks: list[thrml_boost.Block])</code>","text":""},{"location":"api/models/ising/#thrml_boost.models.IsingTrainingSpec","title":"<code>thrml_boost.models.IsingTrainingSpec</code> <code></code>","text":"<p>Contains a complete specification of an Ising EBM that can be trained using sampling-based gradients.</p> <p>Defines sampling programs and schedules that allow for collection of the positive and negative phase samples required for Monte Carlo estimation of the gradient of the KL-divergence between the model and a data distribution.</p>"},{"location":"api/models/ising/#thrml_boost.models.IsingTrainingSpec.__init__","title":"<code>__init__(ebm: thrml_boost.models.IsingEBM, data_blocks: list[thrml_boost.Block], conditioning_blocks: list[thrml_boost.Block], positive_sampling_blocks: list[tuple[thrml_boost.Block, ...] | thrml_boost.Block], negative_sampling_blocks: list[tuple[thrml_boost.Block, ...] | thrml_boost.Block], schedule_positive: thrml_boost.SamplingSchedule, schedule_negative: thrml_boost.SamplingSchedule)</code>","text":""},{"location":"api/models/ising/#thrml_boost.models.hinton_init","title":"<code>thrml_boost.models.hinton_init(key: Key[Array, ''], model: thrml_boost.models.IsingEBM, blocks: list[thrml_boost.Block[thrml_boost.AbstractNode]], batch_shape: tuple[int]) -&gt; list[Bool[Array, 'batch_size block_size']]</code> <code></code>","text":"<p>Initialize the blocks according to the marginal bias.</p> <p>Each binary unit \\(i\\) in a block is sampled independently as</p> \\[\\mathbb{P}(S_i = 1) = \\sigma(\\beta h_i) = \\frac{1}{1 + e^{-\\beta h_i}}\\] <p>where \\(h_i\\) is the bias of unit i and \\(\\beta\\) is the inverse-temperature scaling factor. See Hinton (2012) for a discussion of this initialization heuristic.</p> <p>All blocks are sampled in parallel via <code>vmap</code> over a stacked index array, so the number of XLA ops is O(1) in the number of blocks rather than O(n_blocks). Blocks must all have the same size; empty blocks are rejected by <code>BlockSpec</code> upstream.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Key[Array, '']</code> <p>the JAX PRNG key to use</p> required <code>model</code> <code>thrml_boost.models.IsingEBM</code> <p>the Ising model to initialize for</p> required <code>blocks</code> <code>list[thrml_boost.Block[thrml_boost.AbstractNode]]</code> <p>the blocks that are to be initialized</p> required <code>batch_shape</code> <code>tuple[int]</code> <p>the pre-pended batch dimension</p> required <p>Returns:</p> Type Description <code>list[Bool[Array, 'batch_size block_size']]</code> <p>the initialized blocks as a list of bool arrays, one per block</p>"},{"location":"api/models/ising/#thrml_boost.models.estimate_moments","title":"<code>thrml_boost.models.estimate_moments(key: Key[Array, ''], first_moment_nodes: list[thrml_boost.AbstractNode], second_moment_edges: list[tuple[thrml_boost.AbstractNode, thrml_boost.AbstractNode]], program: thrml_boost.BlockSamplingProgram, schedule: thrml_boost.SamplingSchedule, init_state: list[Array], clamped_data: list[Array])</code> <code></code>","text":"<p>Estimates the first and second moments of an Ising model Boltzmann distribution via sampling.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Key[Array, '']</code> <p>the jax PRNG key</p> required <code>first_moment_nodes</code> <code>list[thrml_boost.AbstractNode]</code> <p>the nodes that represent the variables we want to estimate the first moments of</p> required <code>second_moment_edges</code> <code>list[tuple[thrml_boost.AbstractNode, thrml_boost.AbstractNode]]</code> <p>the edges that connect the variables we want to estimate the second moments of</p> required <code>program</code> <code>thrml_boost.BlockSamplingProgram</code> <p>the <code>BlockSamplingProgram</code> to be used for sampling</p> required <code>schedule</code> <code>thrml_boost.SamplingSchedule</code> <p>the schedule to use for sampling</p> required <code>init_state</code> <code>list[Array]</code> <p>the variable values to use to initialize the sampling</p> required <code>clamped_data</code> <code>list[Array]</code> <p>the variable values to assign to the clamped nodes</p> required <p>Returns:     the first and second moment data</p>"},{"location":"api/models/ising/#thrml_boost.models.estimate_kl_grad","title":"<code>thrml_boost.models.estimate_kl_grad(key: Key[Array, ''], training_spec: thrml_boost.models.IsingTrainingSpec, bias_nodes: list[thrml_boost.AbstractNode], weight_edges: list[tuple[thrml_boost.AbstractNode, thrml_boost.AbstractNode]], data: list[Array], conditioning_values: list[Array], init_state_positive: list[Array], init_state_negative: list[Array]) -&gt; tuple</code> <code></code>","text":"<p>Estimate the KL-gradients of an Ising model with respect to its weights and biases.</p> <p>Uses the standard two-term Monte Carlo estimator of the gradient of the KL-divergence between an Ising model and a data distribution.</p> <p>The gradients are:</p> \\[\\Delta W = -\\beta (\\langle s_i s_j \\rangle_{+} - \\langle s_i s_j \\rangle_{-})\\] \\[\\Delta b = -\\beta (\\langle s_i \\rangle_{+} - \\langle s_i \\rangle_{-})\\] <p>Here, \\(\\langle\\cdot\\rangle_{+}\\) denotes an expectation under the positive phase (data-clamped Boltzmann distribution) and \\(\\langle\\cdot\\rangle_{-}\\) under the negative phase (model distribution).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Key[Array, '']</code> <p>the JAX PRNG key</p> required <code>training_spec</code> <code>thrml_boost.models.IsingTrainingSpec</code> <p>the Ising EBM for which to estimate the gradients</p> required <code>bias_nodes</code> <code>list[thrml_boost.AbstractNode]</code> <p>the nodes for which to estimate the bias gradients</p> required <code>weight_edges</code> <code>list[tuple[thrml_boost.AbstractNode, thrml_boost.AbstractNode]]</code> <p>the edges for which to estimate the weight gradients</p> required <code>data</code> <code>list[Array]</code> <p>The data values to use for the positive phase of the gradient estimate. Each array has shape [batch nodes]</p> required <code>conditioning_values</code> <code>list[Array]</code> <p>values to assign to the nodes that the model is conditioned on. Each array has shape [nodes]</p> required <code>init_state_positive</code> <code>list[Array]</code> <p>initial state for the positive sampling chain. Each array has shape [n_chains_pos batch nodes]</p> required <code>init_state_negative</code> <code>list[Array]</code> <p>initial state for the negative sampling chain. Each array has shape [n_chains_neg nodes]</p> required <p>Returns:     the weight gradients and the bias gradients</p>"},{"location":"examples/00_probabilistic_computing/","title":"Getting Started with THRML","text":"<p>Extropic hardware will become increasingly capable over the next few years. If everything goes to plan, our first ~million variable chips will come online in 2026 and be integrated into large many-chip systems by 2027. These systems will be capable of complex probabilistic computations and could offer a substantial edge to anyone who knows how to wield them.</p> <p>Given these (relatively) short timelines to large-scale commercial viability, businesses must start working today to adapt our tech to their use cases. To enable this, we built a software library that simulates Extropic hardware devices on traditional machine learning accelerators like GPUs. Users can leverage this library to explore how future Extropic systems could be used to accelerate workloads they care about.</p> <p>Our simulation library, THRML, is a GPU simulator of the probabilistic sampling programs that run natively on Extropic hardware. It is built on top of JAX and is massively scalable, enabling users to simulate arbitrarily large probabilistic computing systems given sufficient GPU resources.</p>"},{"location":"examples/00_probabilistic_computing/#whats-a-probabilistic-computer","title":"What's a probabilistic computer?","text":"<p>Probabilistic computers leverage arrays of massively parallel random number generators to sample from distributions that are relevant to solving practical problems. By leveraging physical effects to generate random numbers and emphasising local communication, probabilistic computers can process information extremely efficiently. </p> <p>These sampling-centric computers make direct contact with contemporary machine learning via Energy Based Models (EBMs). Much like popular diffusion models and autoregressive language models, EBMs attempt to learn the probability distribution that underlies some observations of the world. However, EBMs are unique in that they try to directly learn the shape of this distribution instead of attempting to represent it via the composition of many much simpler distributions. This direct fitting of data distributions leads to training and inference pipelines that are extremely sampling-heavy. While these sampling-heavy workloads can challenge traditional machine learning accelerators, they present a massive opportunity for probabilistic computers.</p> <p>In particular, symmetries of certain families of EBMs can be leveraged to map EBM sampling problems directly onto probabilistic computing hardware, unlocking ultra-efficient EBM training and inference. We leveraged these symmetries, along with other hardware and algorithm innovations, to demonstrate that probabilistic computers can be many orders of magnitude more energy-efficient than GPUs in our recent paper.</p>"},{"location":"examples/00_probabilistic_computing/#how-do-extropics-probabilistic-computers-work","title":"How do Extropic's probabilistic computers work?","text":"<p>To truly understand how Extropic's probabilistic computers operate and their utility in machine learning, we must examine the underlying mathematics of EBMs and the algorithms that enable probabilistic computers to sample from them.</p> <p>EBMs attempt to model data distributions by learning a parameterized Energy Function \\(\\mathcal{E}(x, \\theta)\\) that defines the shape of the model's probability distribution,</p> \\[ \\mathbb{P}(X = x) \\propto e^{-\\mathcal{E}(x, \\theta)}\\] <p>Here, \\(X\\) is a vector of random variables that represents the data you want to model, which could be text, images, etc. EBMs are fit by tweaking the model's parameters \\(\\theta\\) to assign low values of energy to values of \\(x\\) that show up in the dataset and large values of energy to values of \\(x\\) that don't. </p> <p>For EBMs, both inference and training involve sampling from this potentially very complex probability distribution, which is very expensive to do on a CPU or GPU. At inference time, we aim to generate new data that resembles the dataset, which requires drawing samples from the learned distribution. During training, to estimate gradients of the typical EBM training objective with respect to the parameters \\(\\theta\\), one needs to compute estimators of certain averages over the EBM's distribution, which means lots of sampling. If a probabilistic computer can make this sampling very efficient, it could dramatically improve the real-world performance of EBMs relative to other types of machine learning models.</p> <p>Extropic's probabilistic computers can efficiently sample from factorized EBMs that contain only local interactions. An EBM is factorized if its energy function splits up into a sum over independent terms,</p> \\[ \\mathcal{E}(x)  = \\sum_{(\\psi, x_1, \\dots, x_N) \\in S} \\psi \\left( x_1, \\dots, x_N \\right) \\] <p>where \\(S\\) is the set of all the factors involved in the EBM. Each member of \\(S\\) consists of a factor energy function \\(\\psi\\) that acts on a subset of the model's variables \\(x_1, \\dots, x_N\\). A factor is local if it only involves variables that are somehow \"close\" to each other, which in the context of a probabilistic computer means that they are embodied by circuitry that lives on nearby parts of a chip.</p> <p>Extropic's probabilistic computers leverage the Gibbs sampling algorithm to efficiently sample from these special EBMs. Gibbs sampling is a procedure in which samples are drawn from the EBM's distribution by iteratively updating blocks of non-interacting variables according to their conditional distributions.</p> \\[ \\mathbb{P}(X_i = x_i| X_{nb(i)} = x') \\propto e^{-\\mathcal{E}_i(x_i, x')}\\] <p>where \\(X_{nb(i)}\\) is the set of random variables that \\(X_i\\) interacts with and \\(\\mathcal{E}_i\\) contains contributions from the set of factors that involve the state of \\(X_i\\), which we denote as \\(S_i\\),</p> \\[ \\mathcal{E}_i(x_i, x') = \\sum_{(\\psi, x_1, \\dots, x_K) \\in S_i} \\psi \\left(x_i, x_1, \\dots, x_K \\right) \\] <p>Extropic's probabilistic computers implement this block Gibbs sampling procedure at the hardware level to dramatically reduce the energy cost of EBM training and inference.</p>"},{"location":"examples/00_probabilistic_computing/#a-concrete-example","title":"A concrete example","text":"<p>To make all of this more concrete, here we will implement a simulation of a simple probabilistic computer sampling from an EBM using THRML.</p> <p>We will implement sampling for a Potts Model, which is a type of EBM that was first developed to study various phenomena in solid-state physics. Potts models have energy functions like,</p> \\[ \\mathcal{E}(x) = -\\beta \\left( \\sum_i W^{(1)}_i [x_i] + \\sum_{(i, j) \\in S} W^{(2)}_{i, j} [x_i, x_j] \\right)\\] <p>Here, \\(x_i\\) is an integer representing a possible state of a categorical random variable. The vector \\(W^{(1)}_{i}\\) induces a bias on the \\(i^{th}\\) variable by adding or subtracting energy based on the value of \\(x_i\\). The matrix \\(W^{(2)}_{i, j}\\) generates an interaction between the \\(i^{th}\\) and \\(j^{th}\\) variables by contributing a term to the energy function that depends on both \\(x_i\\) and \\(x_j\\).</p> <p>Let's use THRML to run block Gibbs sampling on a Potts model.</p> <p>First, some imports that will be useful,</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport networkx as nx\n</code></pre> <pre><code>from thrml_boost.block_management import Block\nfrom thrml_boost.block_sampling import BlockGibbsSpec, sample_states, SamplingSchedule\nfrom thrml_boost.pgm import CategoricalNode\nfrom thrml_boost.models.discrete_ebm import (\n    CategoricalEBMFactor,\n    CategoricalGibbsConditional,\n)\nfrom thrml_boost.factor import FactorSamplingProgram\n</code></pre> <p>The Potts model energy function naturally suggests a graphical interpretation. Namely, we can assign each random variable in the problem to a node, and assign biases to each node to implement the \\(W^{(1)}\\). We can connect variables with edges to represent the interactions \\(W^{(2)}\\).</p> <p>As such, let's define a simple graph to use in our problem,</p> <pre><code>side_length = 20\n\n# in this simple example we will just use a basic grid, although THRML is capable of dealing with arbitrary graph topologies\nG = nx.grid_graph(dim=(side_length, side_length), periodic=False)\n\n# label the nodes with something THRML recognizes for convenience\ncoord_to_node = {coord: CategoricalNode() for coord in G.nodes}\nnx.relabel_nodes(G, coord_to_node, copy=False)\nfor coord, node in coord_to_node.items():\n    G.nodes[node][\"coords\"] = coord\n\n# write down the color groups for later\nbicol = nx.bipartite.color(G)\ncolor0 = [n for n, c in bicol.items() if c == 0]\ncolor1 = [n for n, c in bicol.items() if c == 1]\n\n# write down the edges in a different format for later\nu, v = map(list, zip(*G.edges()))\n\n# plot the graph\npos = {n: G.nodes[n][\"coords\"][:2] for n in G.nodes}\ncolors = [\"black\", \"orange\"]\nnode_colors = [colors[bicol[n]] for n in G.nodes]\n\nfig, axs = plt.subplots()\n\nnx.draw(\n    G,\n    pos=pos,\n    ax=axs,\n    node_size=50.0,\n    node_color=node_colors,\n    edgecolors=\"k\",\n    with_labels=False,\n)\n</code></pre> <p></p> <p>The graph we just drew can be interpreted as a high-level schematic for a probabilistic computer. Each node is associated with specialized circuitry for performing the Gibbs sampling conditional update, which requires state information from neighboring nodes that is communicated along the edges (which could represent physical wires). Nodes of the same color can be updated in parallel, which means that a single iteration of Gibbs sampling can be performed by first updating all of the orange nodes simultaneously, followed by all of the blue nodes.</p> <p>Now that we have our graph, we can define the interactions that make up the energy function we described earlier. THRML comes with a canned implementation of Potts model style interactions in <code>thrml.models</code> that we can leverage here. We will consider a simple case that is commonly studied in physics with no biases and identity coupling matrices.</p> <pre><code># how many categories to use for each variable\nn_cats = 5\n\n# temperature parameter\nbeta = 1.0\n\n\n# implements W^{2} for each edge\n# in this case we are just using an identity matrix, but this could be anything\nid_mat = jnp.eye(n_cats)\nweights = beta * jnp.broadcast_to(jnp.expand_dims(id_mat, 0), (len(u), *id_mat.shape))\ncoupling_interaction = CategoricalEBMFactor([Block(u), Block(v)], weights)\n\ninteractions = [coupling_interaction]\n</code></pre> <p>We can use these interactions to build a sampling program! THRML exposes tools that we can use to very easily run the Gibbs sampling algorithm for our problem.</p> <p>For our Potts model, we will need to update our state using a conditional distribution with the energy function,</p> \\[ \\mathcal{E}_i(x_i, x') = -\\beta \\left( W^{(1)}_i [x_i] + \\sum_{j \\in S_i} W^{(2)}_{i, j} [x_i, x_j] \\right)\\] <p>This corresponds to sampling from a softmax distribution. THRML comes pre-packaged with this conditional, which we will leverage to perform our sampling:</p> <pre><code># first, we have to specify a division of the graph into blocks that will be updated in parallel during gibbs sampling\n# During gibbs sampling, we are only allowed to update nodes in parallel if they are not neighbours\n# Mathematically, this means we should choose our sampling blocks based on the \"minimum coloring\" of the graph\n# we already computed this earlier\n\n# a Block of nodes is simply a sequence of nodes that are all the same type\n# we only have one type of node here, so not important to understand yet\nblocks = [Block(color0), Block(color1)]\n\n# our grouping of the graph into blocks\nspec = BlockGibbsSpec(blocks, [])\n\n# we have to define how each node in our blocks should be updated during each iteration of Gibbs sampling\n# THRML comes with a sampler that will do this for the vanilla potts model we are using here, so lets use that\nsampler = CategoricalGibbsConditional(n_cats)\n\n# now we can make a sampling program, which combines our grouping with the interactions we defined earlier\nprog = FactorSamplingProgram(\n    spec,  # our block decomposition of the graph\n    [\n        sampler for _ in spec.free_blocks\n    ],  # how to update the nodes in each block every iteration of Gibbs sampling\n    interactions,  # the interactions present in our model\n    [],\n)\n</code></pre> <p>That's everything! Now we can simply run our sampling program and observe the results,</p> <pre><code># rng seed\nkey = jax.random.key(4242)\n\n# everything in THRML is completely compatible with standard jax functionality like jit, vmap, etc.\n# here we will use vmap to run a bunch of parallel instances of Gibbs sampling\nn_batches = 100\n\n# we need to initialize our Gibbs sampling instances\ninit_state = []\nfor block in spec.free_blocks:\n    key, subkey = jax.random.split(key, 2)\n    init_state.append(\n        jax.random.randint(\n            subkey,\n            (n_batches, len(block.nodes)),\n            minval=0,\n            maxval=n_cats,\n            dtype=jnp.uint8,\n        )\n    )\n\n# how we should schedule our sampling\nschedule = SamplingSchedule(\n    # how many iterations to do before drawing the first sample\n    n_warmup=0,\n    # how many samples to draw in total\n    n_samples=100,\n    # how many steps to take between samples\n    steps_per_sample=5,\n)\n\nkeys = jax.random.split(key, n_batches)\n\n# now run sampling\nsamples = jax.jit(\n    jax.vmap(lambda i, k: sample_states(k, prog, schedule, i, [], [Block(G.nodes)]))\n)(init_state, keys)\n</code></pre> <p>If we look at the results of our sample, we are able to observe domain formation, which is a very famous property of Potts models. Domain formation happens because our diagonal weight matrix encourages neighbouring variables to match,, leading to the formation of domains where groups of neighbouring variables are all in the same state. </p> <pre><code>to_plot = [0, 7, 21]\n\nfig, axs = plt.subplots(nrows=1, ncols=len(to_plot))\n\nfor i, num in enumerate(to_plot):\n    axs[i].imshow(samples[0][num, -1, :].reshape((side_length, side_length)))\n</code></pre> <p></p> <p>Despite its apparent simplicity, this example gets at the heart of why EBMs can be used as powerful machine learning primitives. Even though our model only involves simple interactions between neighbouring variables, it produces complex emergent long-range correlations that are difficult to predict and understand. By learning the weights of a model like this (instead of just setting them to be the identity matrix), we can take advantage of this capacity for complexity to model complex real-world phenomena, and do so using very little energy by leveraging probabilistic computing hardware.</p> <p>We've only scratched the surface of what can be done with THRML in this simple example. While here we mostly leaned on canned functionality built into THRML, in reality, THRML was designed from the ground up to make it easier for users to implement their own entirely novel probabilistic graphical model block-sampling routines. We designed it this way because probabilistic computing is a rapidly changing field. So it's essential to have flexible tools available to you that minimize exploratory friction.</p> <p>The other examples in this repo will begin to explore some of the more advanced functionality THRML offers. If you are interested in using THRML for your own research, we encourage you to check it out.</p> <p>Suppose you want to learn more about how hardware-compatible EBMs may be applied to real machine learning problems. In that case, you should check out our paper and an implementation of it in <code>THRML</code>. Our primary hope for this library is that it empowers its users to build on our work, developing progressively more complex machine learning systems that increasingly leverage ultra-efficient probabilistic computing.</p>"},{"location":"examples/01_all_of_thrml/","title":"All of THRML","text":"<p>THRML is a simple library for simulating probabilistic computers on GPUs. </p> <p>Concretely, THRML provides tools for GPU accelerating block sampling algorithms on sparse, heterogeneous probabilistic graphical models (PGMs) like the ones that Extropic hardware runs. The primary function of THRML is to be a scaffold that makes it much easier to implement any desired block sampling algorithm than it would be to do so from scratch. As such, this notebook will walk you through the main set of tools that THRML exposes that you can use in your own explorations. </p> <p>We will demonstrate the capabilities of THRML by using it to implement the Gibbs sampling algorithm for a Gaussian PGM.</p> <p>Gibbs sampling is obviously not a practical numerical method for Gaussian sampling in most cases, and should probably instead be handled using the Cholesky decomposition. We implement it here solely for the purposes of demonstrating THRML, which in reality will be used to attack more complex problems that can't be treated analytically.</p> <p>Specifically, in the first part of this example we will consider a PGM that embodies the Gaussian distribution</p> \\[P(x) \\propto e^{-E_G(x)}\\] <p>Where the energy function \\(E_G(x)\\) is,</p> \\[E_G(x) = \\frac{1}{2} \\left(x - \\mu \\right)^T A \\left( x - \\mu \\right) \\] <p>and \\(A = \\Sigma^{-1}\\), where \\(\\Sigma\\) is the covariance matrix of the distribution.</p> <p>We can expand this and write the energy function as a sum of terms,</p> \\[E_G(x) + C = \\frac{1}{2} \\sum_i A_{ii} \\: x_i^2 + \\sum_{j&gt;i} A_{ij} \\: x_i \\: x_j + \\sum_i b_i \\: x_i\\] <p>Where \\(C\\) is a constant independent of \\(x\\), and \\(b = -\\mu^T A\\) is a biasing vector.</p> <p>This form makes the graphical interpretation of the problem clear. Each of the variables \\(x_i\\)  can be represented by a node, and the nonzero matrix elements of \\(A\\) define edges between the nodes. </p> <p>We will use the Gibbs sampling algorithm to draw samples from our Gaussian distribution. To do this, we first identify the distribution of each \\(x_i\\) conditioned on the rest of the graph,</p> \\[P(x_i | x_{nb(i)}) \\propto e^{-E_i (x_i ,x_{nb(i)})}\\] \\[E_i (x_i ,x_{nb(i)}) = \\frac{1}{2} A_{ii} \\: x_i^2 + x_i \\left( \\sum_{j \\in nb(i)} \\: A_{ij} \\: x_j + b_i \\right)\\] <p>where \\(nb(i)\\) indicates the neighbours of node i, which in this case is all j such that \\(A_{ij} \\neq 0\\). This form makes it clear that Gibbs sampling is local, i.e the state of each node is updated using only information about nodes that it is directly connected to.</p> <p>We can write this in a different form that makes it obvious that the conditional is Gaussian,</p> \\[E_i (x_i ,x_{nb(i)}) + D = \\frac{1}{2} (x_i - m_i) A_{ii} (x_i - m_i) \\] \\[ m_i = - \\left( \\sum_{j \\in nb(i)} \\frac{A_{ij}}{A_{ii}} x_j + \\frac{b_i}{A_{ii}} \\right) \\] <p>where \\(D\\) is a constant independent of \\(x_i\\).</p> <p>The Gibbs sampling algorithm works by iteratively updating each of the \\(x_i\\) according to this conditional distribution. In chromatic Gibbs sampling, nodes that belong to the same color group are updated in parallel. This \"blocked\" version is what we will implement here.</p> <p>With the math out of the way, we can proceed with the implementation of our sampling algorithm using THRML. First, let's get some imports out of the way:</p> <pre><code>import random\nfrom collections import defaultdict\nfrom typing import Hashable, Mapping\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nfrom jaxtyping import Array, Key, PyTree\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code>from thrml_boost.block_management import Block\nfrom thrml_boost.block_sampling import (\n    BlockGibbsSpec,\n    BlockSamplingProgram,\n    sample_states,\n    sample_with_observation,\n    SamplingSchedule,\n)\nfrom thrml_boost.conditional_samplers import (\n    _SamplerState,\n    _State,\n    AbstractConditionalSampler,\n)\nfrom thrml_boost.factor import AbstractFactor, FactorSamplingProgram\nfrom thrml_boost.interaction import InteractionGroup\nfrom thrml_boost.models.discrete_ebm import SpinEBMFactor, SpinGibbsConditional\nfrom thrml_boost.observers import MomentAccumulatorObserver\nfrom thrml_boost.pgm import AbstractNode\n</code></pre> <p>Next, we will define our graph. In THRML, nodes that represent random variables with different data types (binary, categorical, continuous, etc.) are identified using distinct classes that inherit from <code>AbstractNode</code>. For our problem we only have one type of node, which we will define now,</p> <pre><code>class ContinuousNode(AbstractNode):\n    pass\n</code></pre> <p>We will now use the existing python graph library NetworkX to construct a grid graph of our nodes.</p> <pre><code>def generate_grid_graph(\n    *side_lengths: int,\n) -&gt; tuple[\n    tuple[list[ContinuousNode], list[ContinuousNode]],\n    tuple[list[ContinuousNode], list[ContinuousNode]],\n    nx.Graph,\n]:\n    G = nx.grid_graph(dim=side_lengths, periodic=False)\n\n    coord_to_node = {coord: ContinuousNode() for coord in G.nodes}\n    nx.relabel_nodes(G, coord_to_node, copy=False)\n\n    for coord, node in coord_to_node.items():\n        G.nodes[node][\"coords\"] = coord\n\n    # an aperiodic grid is always 2-colorable\n    bicol = nx.bipartite.color(G)\n    color0 = [n for n, c in bicol.items() if c == 0]\n    color1 = [n for n, c in bicol.items() if c == 1]\n\n    u, v = map(list, zip(*G.edges()))\n\n    return (bicol, color0, color1), (u, v), G\n\n\ndef plot_grid_graph(\n    G: nx.Graph,\n    bicol: Mapping[Hashable, int],\n    ax: plt.Axes,\n    *,\n    node_size: int = 300,\n    colors: tuple[str, str] = (\"black\", \"orange\"),\n    **draw_kwargs,\n):\n    pos = {n: G.nodes[n][\"coords\"][:2] for n in G.nodes}\n\n    node_colors = [colors[bicol[n]] for n in G.nodes]\n\n    nx.draw(\n        G,\n        pos=pos,\n        ax=ax,\n        node_color=node_colors,\n        node_size=node_size,\n        edgecolors=\"k\",\n        linewidths=0.8,\n        width=1.0,\n        with_labels=False,\n        **draw_kwargs,\n    )\n</code></pre> <pre><code>colors, edges, g = generate_grid_graph(5, 5)\n\nall_nodes = colors[1] + colors[2]\n\nnode_map = dict(zip(all_nodes, list(range(len(all_nodes)))))\n\nfig, axs = plt.subplots()\n\nplot_grid_graph(g, colors[0], axs)\n</code></pre> <p></p> <p>The blue and orange nodes are the two color groups for our grid graph. Any nodes that are the same color will be sampled simultaneously during block sampling.</p> <p>With the graph in hand, we can fully define the distribution we want to sample from by choosing a corresponding inverse covariance matrix and mean vector,</p> <pre><code># Fixed RNG seed for reproducibility\nseed = 4242\nkey = jax.random.key(seed)\n\n# diagonal elements of the inverse covariance matrix\nkey, subkey = jax.random.split(key, 2)\ncov_inv_diag = jax.random.uniform(subkey, (len(all_nodes),), minval=1, maxval=2)\n\n# add an off-diagonal element to the inverse covariance matrix for each edge in the graph\nkey, subkey = jax.random.split(key, 2)\n# make sure the covaraince matrix is PSD\ncov_inv_off_diag = jax.random.uniform(\n    subkey, (len(edges[0]),), minval=-0.25, maxval=0.25\n)\n\n\ndef construct_inv_cov(\n    diag: Array,\n    all_edges: tuple[list[ContinuousNode], list[ContinuousNode]],\n    off_diag: Array,\n):\n    inv_cov = np.diag(diag)\n\n    for n1, n2, cov in zip(*all_edges, off_diag):\n        inv_cov[node_map[n1], node_map[n2]] = cov\n        inv_cov[node_map[n2], node_map[n1]] = cov\n\n    return inv_cov\n\n\n# construct a matrix representation of the inverse covariance matrix for convenience\ninv_cov_mat = construct_inv_cov(cov_inv_diag, edges, cov_inv_off_diag)\n\ninv_cov_mat_jax = jnp.array(inv_cov_mat)\n\n# mean vector\nkey, subkey = jax.random.split(key, 2)\nmean_vec = jax.random.normal(subkey, (len(all_nodes),))\n\n# bias vector\nb_vec = -1 * jnp.einsum(\"ij, i -&gt; j\", inv_cov_mat, mean_vec)\n</code></pre> <p>Now we can construct a program to sample from the distribution we just defined. All block sampling routines follow more or less the same set of steps</p> <ol> <li>Divide your graph into two sets of blocks. The first set, the \"free\" blocks, will be updated during sampling. The second set, the \"clamped\" blocks, will have their nodes fixed to a constant value during sampling. This is often useful. For example, in the case of EBM sampling, this clamping allows for sampling from a distribution conditioned on the clamped nodes.</li> <li>Iteratively update the states of your free blocks. This means:<ol> <li>Initialize the state of each of the free nodes </li> <li>Update the state of each of the free nodes according to some rule. The update rule for each node is some function that takes in a set of parameters and the states of some subset of the other nodes in the graph, and returns an updated state for the node.</li> <li>Make some observation of the current state of the program. This might mean simply writing down the state of some subset of the nodes, or it might mean computing some more complex observable.</li> <li>Repeat steps 2 and 3 until a statisfactory number of observations have been made</li> </ol> </li> </ol> <p>THRML lets you run any version of this procedure that you want while writing minimal amounts of new code. We have to define 3 main things to accomplish this:</p> <ol> <li>A block specification: a division of our problem graph into free and clamped blocks</li> <li>A set of interactions: these allow us to specify what information is required to compute the conditional updates for each node in our graph.</li> <li>Conditional sampling rules: these specify how to update the state of each node in our graph given the interactions that are applicable to that node</li> </ol> <p>First, we will define a block spec for our problem. In our case, we simply want to sample each color group in sequence, and we won't be clamping any of the nodes,</p> <pre><code># a Block is just a list of nodes that are all the same type\n# forcing the nodes in a Block to be of the same type is important for parallelization\nfree_blocks = [Block(colors[1]), Block(colors[2])]\n\n# we won't be clamping anything here, but in principle this could be a list of Blocks just like above\nclamped_blocks = []\n\n# every node in the program has to be assigned a shape and datatype (or PyTree thereof).\n# this is so THRML can build an internal \"global\" representation of the state of the sampling program using a small number of jax arrays\nnode_shape_dtypes = {ContinuousNode: jax.ShapeDtypeStruct((), jnp.float32)}\n\n# our block specification\nspec = BlockGibbsSpec(free_blocks, clamped_blocks, node_shape_dtypes)\n</code></pre> <p>Now the interactions. Our PGM is of the undirected variety, which means that it can be described naturally using the language of Factor Graphs. Deep knowledge of factor graphs and their nomenclature isn't necessary to use THRML; in this context, a Factor is simply an interaction between a set of variables that has no natural direction. </p> <p>Factor graphs can be viewed as hypergraphs where each factor represents a hyperedge connecting multiple variables. Hyperedges in factor graphs can connect any number of variables, allowing for natural representation of higher-order interactions. For example, a three-way interaction term like \\(x_1 x_2 x_3\\) in an energy function corresponds to a single hyperedge (factor) connecting three variable nodes. </p> <p>A nice thing about the Factor formalism is that in the context of Gibbs sampling, the conditional update rule for the \\(i^{th}\\) node depends only on factors that involve \\(x_i\\). This means that given a set of factors for a graph, if we want to update the state of a given node, we only need to consider a small subset of all of the factors that are local to that node. </p> <p>In our case, our energy function can we written as a sum of a bunch of terms, each of which is associated with a factor. There are three distinct types of term in this sum, each of which is associated with a different type of factor:</p> <ol> <li>\\(A_{ii} \\: x_i^2\\)</li> <li>\\(b_i \\: x_i\\)</li> <li>\\(A_{ij} \\: x_i \\: x_j\\)</li> </ol> <p>Each of these factors contributes to our conditional update rule in a different and consistent way. As such, in the context of algorithms like Gibbs sampling, Factors are defined by their ability to produce a set of directed interactions that effect the different nodes they involve in potentially different ways. In the case of our Gaussian sampling problem, our factors generate interactions that are either:</p> <ol> <li>Linear: contribute terms to the energy function like \\(c_i \\: x_i\\), where \\(c_i\\) does not depend on the \"head node\" \\(x_i\\) but may depend on some \"tail nodes\" \\(x_{nb(i)}\\)</li> <li>Quadratic: contribute terms to the energy function like \\(d_i \\: x_i^2\\), where in our case \\(d_i\\) is a constant independent of the state of the sampling program</li> </ol> <p>THRML implements these abstractions directly in code. </p> <p>The most primitive object is the <code>InteractionGroup</code>, which specifies what parametric and state information should be supplied to a given node to compute it's conditional update. An <code>InteractionGroup</code> is composed of a set of interaction parameters, a set of \"head nodes\", and sets of \"tail nodes\". The head nodes are the nodes whose conditional update is effected by the interaction, and the tail nodes specify which neighbouring node states are required to compute the conditional update.</p> <p>THRML also defines Factors via the <code>AbstractFactor</code> interface. In full generality, THRML defines a factor as anything that can be reduced to a set of <code>InteractionGroup</code>s. THRML also defines more specialized factors (like ones that define an energy), however we won't be using those here.</p> <p>We can use these objects to set up our sampling program,</p> <pre><code># these are just arrays that we can identify by type, will be useful later\n\n\nclass LinearInteraction(eqx.Module):\n    \"\"\"An interaction of the form $c_i x_i$.\"\"\"\n\n    weights: Array\n\n\nclass QuadraticInteraction(eqx.Module):\n    \"\"\"An interaction of the form $d_i x_i^2$.\"\"\"\n\n    inverse_weights: Array\n\n\n# now we can set up our three different types of factors\n\n\nclass QuadraticFactor(AbstractFactor):\n    r\"\"\"A factor of the form $w \\: x^2$\"\"\"\n\n    # 1/A_{ii}\n    inverse_weights: Array\n\n    def __init__(self, inverse_weights: Array, block: Block):\n        # in general, a factor is initialized via a list of blocks\n        # these blocks should all have the same number of nodes, and represent groupings of nodes involved in the factor\n        # for example, if a Factor involved 3 nodes, we would initialize it with 3 parallel blocks of equal length\n        super().__init__([block])\n\n        # this array has shape [n], where n is the number of nodes in block\n        self.inverse_weights = inverse_weights\n\n    def to_interaction_groups(self) -&gt; list[InteractionGroup]:\n        # based on our conditional update rule, we can see that we need this to generate a Quadratic interaction with no tail nodes (i.e this interaction has no dependence on the neighbours of x_i)\n\n        # we create an InteractionGroup that implements this interaction\n\n        interaction = InteractionGroup(\n            interaction=QuadraticInteraction(self.inverse_weights),\n            head_nodes=self.node_groups[0],\n            # no tail nodes in this case\n            tail_nodes=[],\n        )\n\n        return [interaction]\n\n\nclass LinearFactor(AbstractFactor):\n    r\"\"\"A factor of the form $w \\: x$\"\"\"\n\n    # b_i\n    weights: Array\n\n    def __init__(self, weights: Array, block: Block):\n        super().__init__([block])\n        self.weights = weights\n\n    def to_interaction_groups(self) -&gt; list[InteractionGroup]:\n        # follows the same pattern as previous, still no tail nodes\n\n        return [\n            InteractionGroup(\n                interaction=LinearInteraction(self.weights),\n                head_nodes=self.node_groups[0],\n                tail_nodes=[],\n            )\n        ]\n\n\nclass CouplingFactor(AbstractFactor):\n    # A_{ij}\n    weights: Array\n\n    def __init__(self, weights: Array, blocks: tuple[Block, Block]):\n        # in this case our factor involves two nodes, so it is initialized with two blocks\n        super().__init__(list(blocks))\n        self.weights = weights\n\n    def to_interaction_groups(self) -&gt; list[InteractionGroup]:\n        # this factor produces interactions that impact both sets of nodes that it touches\n        # i.e if this factor involves a term like w x_1 x_2, it should produce one interaction with weight w that has x_1 as a head node and x_2 as a tail node,\n        # and another interaction with weight w that has x_2 as a head node and x_1 as a tail node\n\n        # if we were sure that x_1 and x_2 were always the same type of node, the two interactions could be part of the same InteractionGroup\n        # we won't worry about that here though\n        return [\n            InteractionGroup(\n                LinearInteraction(self.weights),\n                self.node_groups[0],\n                [self.node_groups[1]],\n            ),\n            InteractionGroup(\n                LinearInteraction(self.weights),\n                self.node_groups[1],\n                [self.node_groups[0]],\n            ),\n        ]\n</code></pre> <p>Now the conditional update the rule. Here, we will define how the relevant interaction and state information should be used to produce an updated state in our iterative sampling algorithm.</p> <pre><code>class GaussianSampler(AbstractConditionalSampler):\n    def sample(\n        self,\n        key: Key,\n        interactions: list[PyTree],\n        active_flags: list[Array],\n        states: list[list[_State]],\n        sampler_state: _SamplerState,\n        output_sd: PyTree[jax.ShapeDtypeStruct],\n    ) -&gt; tuple[Array, _SamplerState]:\n        # this is where the rubber meets the road in THRML\n\n        # this function gets called during block sampling, and must take in information about interactions and neighbour states and produce a state update\n\n        # interactions, active_flags, and states are three parallel lists.\n\n        # each item in interactions is a pytree, for which each array will have shape [n, k, ...].\n        # this is generated by THRML from the set of InteractionGroups that are used to create a sampling program\n        # n is the number of nodes that we are updating in parallel during this call to sample\n        # k is the maximum number of times any node in the block that is being updated shows up as a head node for this interaction\n\n        # each item in active_flags is a boolean array with shape [n, k].\n        # this is padding that is generated internally by THRML based on the graphical structure of the model,\n        # and serves to allow for heterogeneous graph sampling to be vectorized on accelerators that rely on homogeneous data structures\n\n        # each item in states is a list of Pytrees that represents the state of the tail nodes that are relevant to this interaction.\n        # for example, for an interaction with a single tail node that has a scalar dtype, states would be:\n        # [[n, k],]\n\n        bias = jnp.zeros(shape=output_sd.shape, dtype=output_sd.dtype)\n        var = jnp.zeros(shape=output_sd.shape, dtype=output_sd.dtype)\n\n        # loop through all of the available interactions and process them appropriately\n\n        # here we are simply implementing the math of our conditional update rule\n\n        for active, interaction, state in zip(active_flags, interactions, states):\n            if isinstance(interaction, LinearInteraction):\n                # if there are tail nodes, contribute w * x_1 * x_2 * ..., otherwise contribute w\n                state_prod = jnp.array(1.0)\n                if len(state) &gt; 0:\n                    state_prod = jnp.prod(jnp.stack(state, -1), -1)\n                bias -= jnp.sum(interaction.weights * active * state_prod, axis=-1)\n\n            if isinstance(interaction, QuadraticInteraction):\n                # this just sets the variance of the output distribution\n                # there should never be any tail nodes\n\n                var = active * interaction.inverse_weights\n                var = var[..., 0]  # there should only ever be one\n\n        return (jnp.sqrt(var) * jax.random.normal(key, output_sd.shape)) + (\n            bias * var\n        ), sampler_state\n\n    def init(self) -&gt; _SamplerState:\n        return None\n</code></pre> <p>With all of the parts fully defined, we can now construct our sampling program</p> <pre><code># our three types of factor\nlin_fac = LinearFactor(b_vec, Block(all_nodes))\nquad_fac = QuadraticFactor(1 / cov_inv_diag, Block(all_nodes))\npair_quad_fac = CouplingFactor(cov_inv_off_diag, (Block(edges[0]), Block(edges[1])))\n\n# an instance of our conditional sampler\nsampler = GaussianSampler()\n\n# the sampling program itself. Combines the three main components we just built\nprog = FactorSamplingProgram(\n    gibbs_spec=spec,\n    # one sampler for every free block in gibbs_spec\n    samplers=[sampler, sampler],\n    factors=[lin_fac, quad_fac, pair_quad_fac],\n    other_interaction_groups=[],\n)\n</code></pre> <p><code>FactorSamplingProgram</code> is a thin wrapper on the more generic <code>BlockSamplingProgram</code>. All <code>FactorSamplingProgram</code> does is convert all of the factors passed in into <code>InteractionGroups</code> and then use them to create a `BlockSamplingProgram'. As such, prog is equivalent to prog_2 in the following:</p> <pre><code>groups = []\nfor fac in [lin_fac, quad_fac, pair_quad_fac]:\n    groups += fac.to_interaction_groups()\n\nprog_2 = BlockSamplingProgram(\n    gibbs_spec=spec, samplers=[sampler, sampler], interaction_groups=groups\n)\n</code></pre> <p>Now we are finally ready to do some sampling! A sampling program in THRML simply repeatedly updates the state of each free block in the order they appear in the gibbs_spec. After every iteration of the sampling algorithm, we may observe the state and write down some information that is relevant to the problem we are trying to solve. For example, if we wanted to extract samples from some subset of the nodes of our PGM, after each iteration we could simply memorize some subset of the current state. This functionality is provided by observers in THRML.</p> <p>For the purposes of this example, it would be prudent to check that our sampling program is working correctly. To do this, we will compute estimators of some first and second moments and verify that they match up with expected values from the theory. We will use the built-in <code>MomentAccumulatorObserver</code> to accomplish this.</p> <pre><code># we will estimate the covariances for each pair of nodes connected by an edge and compare against theory\n# to do this we will need to estimate first moments and second moments\nsecond_moments = [(e1, e2) for e1, e2 in zip(*edges)]\nfirst_moments = [[(x,) for x in y] for y in edges]\n\n# this will accumulate products of the node state specified by first_moments and second_moments\nobserver = MomentAccumulatorObserver(first_moments + [second_moments])\n</code></pre> <p>Now all that is left to do is specify a few more details about how the sampling should proceed. </p> <pre><code># how many parallel sampling chains will we run?\nn_batches = 1000\n\n\nschedule = SamplingSchedule(\n    # how many iterations to do before drawing the first sample\n    n_warmup=0,\n    # how many samples to draw in total\n    n_samples=10000,\n    # how many steps to take between samples\n    steps_per_sample=5,\n)\n\n# construct the initial state of the iterative sampling algorithm\ninit_state = []\nfor block in spec.free_blocks:\n    key, subkey = jax.random.split(key, 2)\n    init_state.append(\n        0.1\n        * jax.random.normal(\n            subkey,\n            (\n                n_batches,\n                len(block.nodes),\n            ),\n        )\n    )\n\n# RNG keys to use for each chain in the batch\nkeys = jax.random.split(key, n_batches)\n\n# memory to hold our moment values\ninit_mem = observer.init()\n</code></pre> <p>Now run the sampling:</p> <pre><code># we use vmap to run a bunch of parallel sampling chains\nmoments, _ = jax.vmap(\n    lambda k, s: sample_with_observation(k, prog, schedule, s, [], init_mem, observer)\n)(keys, init_state)\n\n# Take a mean over the batch axis and divide by the total number of samples\nmoments = jax.tree.map(lambda x: jnp.mean(x, axis=0) / schedule.n_samples, moments)\n\n# compute the covariance values from the moment data\ncovariances = moments[-1] - (moments[0] * moments[1])\n</code></pre> <p>We can compare our covariance estimates to the real covariance matrix to see if we implemented our sampling routine correctly</p> <pre><code>cov = np.linalg.inv(inv_cov_mat)\n\nnode_map = dict(zip(all_nodes, list(range(len(all_nodes)))))\n\nreal_covs = []\n\nfor edge in zip(*edges):\n    real_covs.append(cov[node_map[edge[0]], node_map[edge[1]]])\n\nreal_covs = np.array(real_covs)\n\nerror = np.max(np.abs(real_covs - covariances)) / np.abs(np.max(real_covs))\n\nprint(error)\nassert error &lt; 0.01\n</code></pre> <pre><code>0.0045360937\n</code></pre> <p>We achieve a really small error because we computed a ton of samples. If you reduce either the batch size or the number of samples collected by each chain this number will go up.</p> <p>That is everything you need to know to implement any type of PGM block sampling routine you want in THRML. </p> <p>However, you don't always have to do everything completely from scratch! THRML exposes a limited set of higher-level functionality fine-tuned to sampling problems that Extropic really cares about. </p> <p>Next, we will use some of these higher-level functions to implement a more complicated type of model that can't be sampled from using analytical techniques. In particular, we will implement sampling from a deep Gaussian-Bernoulli EBM. This  type of model has the energy function</p> \\[ E(x) = E_G(x) + E_{GB}(x, s) + E_B(s)\\] <p>where \\(x\\) is a vector of continuous values and \\(s\\) is a vector of spins, \\(s_i \\in \\{-1, 1\\}\\).</p> <p>\\(E_G(x)\\) is the Gaussian energy function defined in the previous section. \\(E_{GB}\\) is an energy function that represents the interaction between the continuous and spin-valued variables,</p> \\[ E_{GB}(x, s) = \\sum_{ (i, j) \\in S_{GB}} W_{ij} \\: y_i \\: x_j \\] <p>where \\(S_{GB}\\) is a set of edges connecting spin and continuous variables.</p> <p>\\(E_{B}\\) is the spin energy function,</p> \\[ E_B(s) = \\sum_i b_i \\: s_i + \\sum_{j &gt; i} J_{ij} s_i s_j\\] <p>Just for fun, lets use a more complicated graph topology for this problem. We will stick with a grid, but we will add skip-connections that allow for non-nearest-neighbour interactions. We can once again use NetworkX to make this graph,</p> <pre><code># first, define a new type of node\n\n\nclass SpinNode(AbstractNode):\n    pass\n</code></pre> <pre><code># now, build a random grid out of spin and continuous nodes\n\n\ndef make_random_typed_grid(\n    rows: int,\n    cols: int,\n    seed: int,\n    p_cont: float = 0.5,\n):\n    rng = random.Random(seed)\n\n    # every time we make a node, flip a coin to decide what type it should be\n    grid = [\n        [ContinuousNode() if rng.random() &lt; p_cont else SpinNode() for _ in range(cols)]\n        for _ in range(rows)\n    ]\n\n    # Parity-based 2-coloring\n    bicol = {grid[r][c]: ((r + c) &amp; 1) for r in range(rows) for c in range(cols)}\n\n    # Separate by color and type\n    colors_by_type = {\n        0: {SpinNode: [], ContinuousNode: []},\n        1: {SpinNode: [], ContinuousNode: []},\n    }\n    for r in range(rows):\n        for c in range(cols):\n            n = grid[r][c]\n            color = bicol[n]\n            colors_by_type[color][type(n)].append(n)\n\n    return grid, colors_by_type\n\n\ngrid, coloring = make_random_typed_grid(30, 30, seed)\n</code></pre> <pre><code># now generate the edges to implement our desired skip-connected grid\n# we will use only odd-length edges (1, 3, 5, ...) so that our 2-coloring remains valid\ndef build_skip_graph_from_grid(\n    grid: list[list[AbstractNode]],\n    skips: list[int],\n):\n    rows, cols = len(grid), len(grid[0])\n\n    # Build graph &amp; annotate nodes with coords and type\n    G = nx.Graph()\n    for r in range(rows):\n        for c in range(cols):\n            n = grid[r][c]\n            G.add_node(n, coords=(r, c))\n\n    # Edges sorted by edge length\n    u_all = []\n    v_all = []\n    for k in skips:\n        # vertical: (r, c) -&gt; (r+k, c)\n        for r in range(rows - k):\n            r2 = r + k\n            for c in range(cols):\n                n1 = grid[r][c]\n                n2 = grid[r2][c]\n                u_all.append(n1)\n                v_all.append(n2)\n                G.add_edge(n1, n2)\n\n        # horizontal: (r, c) -&gt; (r, c+k)\n        for r in range(rows):\n            for c in range(cols - k):\n                c2 = c + k\n                n1 = grid[r][c]\n                n2 = grid[r][c2]\n                u_all.append(n1)\n                v_all.append(n2)\n                G.add_edge(n1, n2, skip=k)\n\n    return (u_all, v_all), G\n\n\nedge_lengths = [1, 3, 5]\nedges, graph = build_skip_graph_from_grid(grid, edge_lengths)\n</code></pre> <p>Let's visualize this graph to understand what we just created. Since the graph is no longer planar, it will be cleanest to plot the local neighbourhood of particular nodes in our grid one at a time.</p> <pre><code>def plot_node_neighbourhood(\n    grid,\n    G: nx.Graph,\n    center: Hashable,\n    hops: int,\n    ax: plt.Axes,\n) -&gt; None:\n    rows, cols = len(grid), len(grid[0])\n    r, c = G.nodes[center][\"coords\"]\n\n    # make a rectangular subgrid\n    r0, r1 = max(0, r - hops), min(rows - 1, r + hops)\n    c0, c1 = max(0, c - hops), min(cols - 1, c + hops)\n    rect_nodes = {grid[i][j] for i in range(r0, r1 + 1) for j in range(c0, c1 + 1)}\n\n    # collect the relevant edges by length\n    edges_by_k = defaultdict(list)\n    for v, ed in G[center].items():\n        k = int(ed.get(\"skip\", 1))\n        edges_by_k[k].append((center, v))\n\n    # draw edges as arcs\n    max_k = max(edges_by_k.keys(), default=1)\n    curve_scale = 0.8\n    edge_width = 1.0\n    alpha = 1.0\n\n    def rad_for_edge(u, v, k):\n        r1, c1 = G.nodes[u][\"coords\"]\n        r2, c2 = G.nodes[v][\"coords\"]\n        base = curve_scale * (k / max_k)\n        # choose bend direction based on quadrant:\n        if c1 == c2:\n            sign = +1.0 if r2 &lt; r1 else -1.0  # up vs down\n        else:  # horizontal edge\n            sign = +1.0 if c2 &gt; c1 else -1.0  # right vs left\n        return sign * base\n\n    # positions for plotting\n    pos = {\n        n: (G.nodes[n][\"coords\"][1], G.nodes[n][\"coords\"][0])\n        for n in rect_nodes | {center}\n    }\n\n    for i, k in enumerate(sorted(edges_by_k)):\n        for u, v in edges_by_k[k]:\n            nx.draw_networkx_edges(\n                G,\n                pos,\n                edgelist=[(u, v)],\n                ax=ax,\n                edge_color=\"gray\",\n                width=edge_width,\n                alpha=alpha,\n                arrows=True,\n                arrowstyle=\"-\",\n                connectionstyle=f\"arc3,rad={rad_for_edge(u, v, k)}\",\n            )\n\n    # draw nodes\n    cont_nodes = [n for n in rect_nodes if n.__class__ == ContinuousNode]\n    spin_nodes = [n for n in rect_nodes if n.__class__ == SpinNode]\n\n    node_size = 20.0\n\n    nx.draw_networkx_nodes(\n        G,\n        pos,\n        nodelist=cont_nodes,\n        node_color=\"black\",\n        node_shape=\"s\",\n        node_size=node_size,\n        ax=ax,\n    )\n    nx.draw_networkx_nodes(\n        G,\n        pos,\n        nodelist=spin_nodes,\n        node_color=\"orange\",\n        node_shape=\"o\",\n        node_size=node_size,\n        ax=ax,\n    )\n\n\n# pick a few nodes in the grid to inspect\ncenters = [grid[0][7], grid[10][10], grid[-1][-1]]\n\nfig, axs = plt.subplots(nrows=1, ncols=len(centers), figsize=(len(centers) * 5, 5))\n\n\nfor ax, center in zip(axs, centers):\n    plot_node_neighbourhood(grid, graph, center, max(edge_lengths) + 1, ax)\n</code></pre> <p></p> <p>This problem is clearly much more heterogeneous than what we were looking at before. Every node has a unique local neighbourhood, and is connected to a potentially different number of spin and continuous nodes. This makes working with this graph on an accelerator like a GPU tricky. As we will now see, THRML was specifically designed to handle this heterogeneity.</p> <p>With our graph in hand, let's set up our sampling program. We can re-use a lot of the work that we did in the simpler example. First, let's sort the nodes and edges by type. </p> <pre><code># collect the different types of nodes\nspin_nodes = []\ncont_nodes = []\nfor node in graph.nodes:\n    if isinstance(node, SpinNode):\n        spin_nodes.append(node)\n    else:\n        cont_nodes.append(node)\n\n\n# spin-spin interactions\nss_edges = [[], []]\n\n# continuous-continuous interactions\ncc_edges = [[], []]\n\n# spin-continuous interactions\nsc_edges = [[], []]\n\nfor edge in zip(*edges):\n    if isinstance(edge[0], SpinNode) and isinstance(edge[1], SpinNode):\n        ss_edges[0].append(edge[0])\n        ss_edges[1].append(edge[1])\n    elif isinstance(edge[0], ContinuousNode) and isinstance(edge[1], ContinuousNode):\n        cc_edges[0].append(edge[0])\n        cc_edges[1].append(edge[1])\n    elif isinstance(edge[0], SpinNode):\n        sc_edges[0].append(edge[0])\n        sc_edges[1].append(edge[1])\n    else:\n        sc_edges[1].append(edge[0])\n        sc_edges[0].append(edge[1])\n</code></pre> <p>Now we can set up some interactions. For some of the factors, we will re-use our code from the first part of this example </p> <pre><code># we will just randomize the weights\n\nkey, subkey = jax.random.split(key, 2)\ncont_quad = QuadraticFactor(\n    jax.random.uniform(subkey, (len(cont_nodes),), minval=2, maxval=3),\n    Block(cont_nodes),\n)\n\nkey, subkey = jax.random.split(key, 2)\ncont_linear = LinearFactor(\n    jax.random.normal(subkey, (len(cont_nodes),)), Block(cont_nodes)\n)\n\nkey, subkey = jax.random.split(key, 2)\ncont_coupling = CouplingFactor(\n    jax.random.uniform(subkey, (len(cc_edges[0]),), minval=-1 / 10, maxval=1 / 10),\n    (Block(cc_edges[0]), Block(cc_edges[1])),\n)\n\nkey, subkey = jax.random.split(key, 2)\nspin_con_coupling = CouplingFactor(\n    jax.random.normal(subkey, (len(sc_edges[0]),)),\n    (Block(sc_edges[0]), Block(sc_edges[1])),\n)\n</code></pre> <p>For the factors that involve only spin variables, we will use some built in functionality from THRML. THRML implements sampling functionality for arbitrary discrete-variable EBMs in <code>thrml.models.discrete_ebm</code> that we can apply to our problem. First, the spin factors,</p> <pre><code>key, subkey = jax.random.split(key, 2)\nspin_linear = SpinEBMFactor(\n    [Block(spin_nodes)], jax.random.normal(subkey, (len(spin_nodes),))\n)\n\nkey, subkey = jax.random.split(key, 2)\nspin_coupling = SpinEBMFactor(\n    [Block(x) for x in ss_edges], jax.random.normal(subkey, (len(ss_edges[0]),))\n)\n</code></pre> <p>The Gaussian sampler we wrote for the first part will work our new problem as it is because it won't be seeing any new types of interactions. The Binary sampler built into THRML will have to be extended to handle our <code>LinearInteraction</code>. Luckily, it was designed with this kind of modification in mind.</p> <pre><code>class ExtendedSpinGibbsSampler(SpinGibbsConditional):\n    def compute_parameters(\n        self,\n        key: Key,\n        interactions: list[PyTree],\n        active_flags: list[Array],\n        states: list[list[_State]],\n        sampler_state: _SamplerState,\n        output_sd: PyTree[jax.ShapeDtypeStruct],\n    ) -&gt; PyTree:\n        field = jnp.zeros(output_sd.shape, dtype=float)\n\n        unprocessed_interactions = []\n        unprocessed_active = []\n        unprocessed_states = []\n\n        for interaction, active, state in zip(interactions, active_flags, states):\n            # if its our new interaction, handle it\n            if isinstance(interaction, LinearInteraction):\n                state_prod = jnp.prod(jnp.stack(state, -1), -1)\n                field -= jnp.sum(interaction.weights * active * state_prod, axis=-1)\n\n            # if we haven't seen it, remember it\n            else:\n                unprocessed_interactions.append(interaction)\n                unprocessed_active.append(active)\n                unprocessed_states.append(state)\n\n        # make the parent class deal with THRML-native interactions\n        field -= super().compute_parameters(\n            key,\n            unprocessed_interactions,\n            unprocessed_active,\n            unprocessed_states,\n            sampler_state,\n            output_sd,\n        )[0]\n\n        return field, sampler_state\n</code></pre> <p>This is all the work we need to do to sample from our new graph using THRML! All that is left is to set up our Block spec and run some sampling.</p> <pre><code># tell THRML the shape and datatype of our new node\nnew_sd = {SpinNode: jax.ShapeDtypeStruct(shape=(), dtype=jnp.bool)}\n\n# Our new graph is still two-colorable, however within each color there are two different types of node\n# this means that we can't make a single block to represent each color because all of the nodes within a block have to be of the same type\n# however, we might still want to ensure that the two blocks that represent each color group are sampled at the same \"algorithmic\" time\n# i.e even though we can't sample these blocks directly in parallel because they use different update rules, we want to make sure that they\n# receive the same state information\n# we can make this happen in THRML by passing in a list of tuples of blocks to BlockGibbsSpec instead of a list of Blocks\n# the blocks in each tuple will be sampled at the same algorithmic time\nblocks = [\n    (Block(coloring[0][SpinNode]), Block(coloring[0][ContinuousNode])),\n    (Block(coloring[1][SpinNode]), Block(coloring[1][ContinuousNode])),\n]\n\nblock_spec = BlockGibbsSpec(blocks, [], node_shape_dtypes | new_sd)\n\n# now we can assemble our program\n\n# first, choose the right update rule for each block in the spec\nber_sampler = ExtendedSpinGibbsSampler()\nsamplers = []\nfor block in block_spec.free_blocks:\n    if isinstance(block.nodes[0], SpinNode):\n        samplers.append(ber_sampler)\n    else:\n        samplers.append(sampler)\n\n# collect all of our factors\nfactors = [\n    cont_quad,\n    cont_linear,\n    cont_coupling,\n    spin_con_coupling,\n    spin_linear,\n    spin_coupling,\n]\n\nprogram = FactorSamplingProgram(block_spec, samplers, factors, [])\n</code></pre> <p>Our program is doing a lot of work to pad out the interaction structure and make our sampling program GPU-compatible:</p> <pre><code># let's look at an example of the padding\nprogram.per_block_interaction_active[0][0][0]\n</code></pre> <pre><code>Array([ True,  True,  True,  True,  True, False, False, False, False,\n       False], dtype=bool)\n</code></pre> <p>Now we are ready to sample. In this case, we will simply observe the state of our nodes directly</p> <pre><code>batch_size = 50\n\nschedule = SamplingSchedule(\n    # how many iterations to do before drawing the first sample\n    n_warmup=100,\n    # how many samples to draw in total\n    n_samples=300,\n    # how many steps to take between samples\n    steps_per_sample=15,\n)\n\n\n# construct the initial state of the iterative sampling algorithm\ninit_state = []\nfor block in block_spec.free_blocks:\n    init_shape = (\n        batch_size,\n        len(block.nodes),\n    )\n    key, subkey = jax.random.split(key, 2)\n    if isinstance(block.nodes[0], ContinuousNode):\n        init_state.append(0.1 * jax.random.normal(subkey, init_shape))\n    else:\n        init_state.append(jax.random.bernoulli(subkey, 0.5, init_shape))\n\nkey, subkey = jax.random.split(key, 2)\nkeys = jax.random.split(subkey, batch_size)\n\nsamples = jax.vmap(\n    lambda k, i: sample_states(\n        k, program, schedule, i, [], [Block(spin_nodes), Block(cont_nodes)]\n    )\n)(keys, init_state)\n</code></pre> <p>Let's visualize our samples. Our data is very high-dimensional, but we can use a PCA to try and get some idea of the structure of the distribution.</p> <pre><code>all_samples = jnp.concatenate(samples, axis=-1)\npca = PCA(n_components=3)\npreproc_data = StandardScaler().fit_transform(\n    jnp.reshape(all_samples, (-1, all_samples.shape[-1]))\n)\ntransformed_data = pca.fit_transform(preproc_data)\n</code></pre> <pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nsc = ax.scatter(\n    transformed_data[:, 0],  # PC1\n    transformed_data[:, 1],  # PC2\n    transformed_data[:, 2],  # PC3\n    s=50,\n    alpha=0.8,\n)\nax.view_init(elev=-50, azim=280)\nplt.show()\n</code></pre> <p></p> <p>This distribution is clearly non-Gaussian and complex, despite the random initialization.</p> <p>If you've made it to the end of this example and have been paying attention you are now ready to use THRML for your own research-grade problems! We are very excited to see what you make with it.</p>"},{"location":"examples/02_spin_models/","title":"Spin Models in THRML","text":"<p>Probabilistic computers that sample from graphical models defined over binary random variables are most natural to build using transistors, and therefore are of elevated interest to Extropic. As such, we've built some tooling into THRML that is dedicated to sampling from these binary PGMs and training machine learning models based on them. This notebook will walk through this functionality and show you how to use it.</p> <p>We specifically consider spin-valued EBMs with polynomial interactions. These models implement the probability distribution,</p> \\[ P(x) \\propto e^{-\\mathcal{E}(x)}\\] \\[ \\mathcal{E}(x) = -\\beta \\left( \\sum_{i \\in S_1} W^{(1)}_i s_i + \\sum_{(i, j) \\in S_2} W^{(2)}_{i, j} s_i s_j +  \\sum_{(i, j, k) \\in S_3} W^{(3)}_{i, j, k} s_i s_j s_k  + \\dots \\right) \\] <p>Here, the \\(s_i \\in \\{-1, 1\\}\\) are spin variables that couple with each other via the \\(W^{(k)}\\), which are scalars that represent the strengths of \\(k^{th}\\) order interactions. \\(S_k\\) is the set of all interactions of order \\(k\\).</p> <p>A model of this type that contains at most second-order interactions is called an Ising model or Boltzmann machine. Boltzmann machines are one of the original machine learning models, and their significance was recognized in 2024 with a Nobel prize in physics for John Hopfield and Geoffrey Hinton. </p> <p>Gibbs sampling defines a simple procedure for sampling from this type of model that is very hardware friendly. In particular, the Gibbs sampling update rule corresponding to the above energy function is,</p> \\[ P(s_i = 1 | s_{nb(i)}) = \\sigma[2 \\gamma]\\] \\[ \\gamma = W^{(1)}_i +  \\sum_{j \\in S_2[i]} W^{(2)}_{i, j} s_j + \\sum_{(j, k) \\in S_3[i]} W^{(3)}_{i, j, k} s_j s_k + \\dots\\] <p>where \\(s_{nb(i)}\\) are the spins that are neighbours of \\(s_i\\), and  \\(S_k[i]\\) is the members of \\(S_k\\) that contain \\(i\\).</p> <p>From the above equation, we see that we can implement the Gibbs sampling update rule for a spin-valued model by computing simple functions of the neighbour states, multiply-accumulating the results, and then using them to generate an appropriately biased random bit. This can be done very efficiently using mixed signal (analog + digital) hardware; we flesh out a way to do this using only transistors on a modern process in our recent paper.</p> <p>Now that we understand the significance of this type of model, let's see how they can be sampled from using some of the tools built in to THRML.</p> <p>First, some imports,</p> <pre><code>import time\nimport jax\n\nimport dwave_networkx\nimport jax.numpy as jnp\nimport jax.random\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\n</code></pre> <pre><code>from thrml_boost.block_management import Block\nfrom thrml_boost.block_sampling import sample_states, SamplingSchedule\nfrom thrml_boost.models.discrete_ebm import SpinEBMFactor\nfrom thrml_boost.models.ising import (\n    estimate_kl_grad,\n    hinton_init,\n    IsingEBM,\n    IsingSamplingProgram,\n    IsingTrainingSpec,\n)\nfrom thrml_boost.pgm import SpinNode\n</code></pre> <p>In this example, we will implement a quadratic binary model (Ising model). We will use DWave's \"Pegasus\" graph topology to allow us to directly compare the speed of our GPU-based sampler to results obtained using other hardware accelerators,</p> <pre><code># make the graph using DWave's code\ngraph = dwave_networkx.pegasus_graph(14)\ncoord_to_node = {coord: SpinNode() for coord in graph.nodes}\nnx.relabel_nodes(graph, coord_to_node, copy=False)\n</code></pre> <pre><code>&lt;networkx.classes.graph.Graph at 0x7c2aea39ba30&gt;\n</code></pre> <p>Now we can define our model using the functionality exposed by <code>thrml.models.ising</code>. For the sake of this example, we will choose random values for the biases and weights \\(W^{(1)}\\) and \\(W^{(2)}\\),</p> <pre><code>nodes = list(graph.nodes)\nedges = list(graph.edges)\n\nseed = 4242\nkey = jax.random.key(seed)\n\nkey, subkey = jax.random.split(key, 2)\nbiases = jax.random.normal(subkey, (len(nodes),))\n\nkey, subkey = jax.random.split(key, 2)\nweights = jax.random.normal(subkey, (len(edges),))\n\nbeta = jnp.array(1.0)\n\nmodel = IsingEBM(nodes, edges, biases, weights, beta)\n</code></pre> <p>The <code>IsingEBM</code> class is simply a thin frontend that takes in your weights and biases and produces an appropriate set of <code>SpinEBMFactor</code>s,</p> <pre><code>[x.__class__ for x in model.factors]\n</code></pre> <pre><code>[thrml.models.discrete_ebm.SpinEBMFactor,\n thrml.models.discrete_ebm.SpinEBMFactor]\n</code></pre> <p>Now let's do some computation using our <code>IsingEBM</code>. Specifically, we are going to look at the tools THRML exposes for training this type of model in the context of machine learning. In machine learning, the variables in an EBM are often segmented into \"visible\" variables (x) and \"latent\" variables (z). The visible variables represent the data, and the latent variables serve to increase the expressivity of the model. Given these latent variables, our EBMs model of the data is,</p> \\[ P(x) \\propto \\sum_z e^{-\\mathcal{E}(x, z)}\\] <p>When training EBMs, one is often interested in minimizing the distributional distance between the EBM and some dataset. This can be done by iteratively updating the model parameters according to the gradient,</p> \\[ \\nabla_{\\theta} D(Q(x)|| P(x)) = \\mathbb{E}_Q \\left[ \\mathbb{E}_{P(z|x)} \\left[ \\nabla_{\\theta} \\mathcal{E}\\right] - \\mathbb{E}_{P(z, \\: x)} \\left[ \\nabla_{\\theta} \\mathcal{E}\\right] \\right]\\] <p>Where \\(D(Q||P)\\) indicates the KL-divergence between Q and P, which is a common measure of distributional distance in machine learning. Each of the two terms in this gradient can be estimated by sampling from the EBM. The first term is estimated by clamping the data nodes to a member of the dataset and sampling the latents. The second is estimated by sampling both the data and latent variables. We can leverage THRML for both of these computations.</p> <p>First, lets set up our block specifications for both the free and clamped sampling. First, lets choose some random subset of our nodes to represent the data,</p> <pre><code>n_data = 500\n\nnp.random.seed(seed)\n\ndata_inds = np.random.choice(len(graph.nodes), n_data, replace=False)\ndata_nodes = [nodes[x] for x in data_inds]\n</code></pre> <p>Now, lets compute the minimum coloring for the unclamped term in our gradient estimator,</p> <pre><code>coloring = nx.coloring.greedy_color(graph, strategy=\"DSATUR\")\nn_colors = max(coloring.values()) + 1\nfree_coloring = [[] for _ in range(n_colors)]\n# form color groups\nfor node in graph.nodes:\n    free_coloring[coloring[node]].append(node)\n\nfree_blocks = [Block(x) for x in free_coloring]\n</code></pre> <p>and the same for the clamped term,</p> <pre><code># in this case we will just re-use the free coloring\n# you can always do this, but it might not be optimal\n\n# a graph without the data nodes\ngraph_copy = graph.copy()\ngraph_copy.remove_nodes_from(data_nodes)\n\nclamped_coloring = [[] for _ in range(n_colors)]\nfor node in graph_copy.nodes:\n    clamped_coloring[coloring[node]].append(node)\n\nclamped_blocks = [Block(x) for x in clamped_coloring]\n</code></pre> <p>We have now defined everything we need to calculate some gradients! We can set up a few more details and get to it,</p> <pre><code># lets define some random \"data\" to use for our example\n# in real life this could be encoded images, text, video etc\ndata_batch_size = 50\n\nkey, subkey = jax.random.split(key, 2)\ndata = jax.random.bernoulli(subkey, 0.5, (data_batch_size, len(data_nodes))).astype(\n    jnp.bool\n)\n\n# we will use the same sampling schedule for both cases\nschedule = SamplingSchedule(5, 100, 5)\n\n# convenient wrapper for everything you need for training\ntraining_spec = IsingTrainingSpec(\n    model, [Block(data_nodes)], [], clamped_blocks, free_blocks, schedule, schedule\n)\n\n# how many parallel sampling chains to run for each term\nn_chains_free = data_batch_size\nn_chains_clamped = 1\n\n# initial states for each sampling chain\n# THRML comes with simple code for implementing the hinton initialization, which is commonly used with boltzmann machines\nkey, subkey = jax.random.split(key, 2)\ninit_state_free = hinton_init(subkey, model, free_blocks, (n_chains_free,))\nkey, subkey = jax.random.split(key, 2)\ninit_state_clamped = hinton_init(\n    subkey, model, clamped_blocks, (n_chains_clamped, data_batch_size)\n)\n</code></pre> <pre><code># now for gradient estimation!\n# this function returns the gradient estimators for the weights and edges of our model, along with the moment data that was used to estimate them\n# the moment data is also returned in case you want to use it for something else in your training loop\nkey, subkey = jax.random.split(key, 2)\nweight_grads, bias_grads, clamped_moments, free_moments = estimate_kl_grad(\n    subkey,\n    training_spec,\n    nodes,  # the nodes for which to compute bias gradients\n    edges,  # the edges for which to compute weight gradients\n    [data],\n    [],\n    init_state_clamped,\n    init_state_free,\n)\n</code></pre> <p>This function simply returns vectors for the weight and bias grads,</p> <pre><code>print(weight_grads)\nprint(bias_grads)\n</code></pre> <pre><code>[ 0.7848     -0.33560008 -0.148      ...  0.00640005 -0.15759999\n -0.01319999]\n[0.43279994 1.1767999  0.04360002 ... 0.01319999 0.18919998 0.14079998]\n</code></pre> <p>which can be used to train your model using whatever outer loop code you want!</p> <p>Because THRML is written in jax, it runs sampling programs very efficiently on GPUs and is competitive with the state of the art for sampling from sparse Ising models. Let's demonstrate that with a simple benchmark,</p> <p>Warning</p> <p>The following requires 8x GPUs.</p> <pre><code>from jax.sharding import PartitionSpec as P\n</code></pre> <pre><code>mesh = jax.make_mesh((8,), (\"x\",))\nsharding = jax.sharding.NamedSharding(mesh, P(\"x\"))\n\ntiming_program = IsingSamplingProgram(model, free_blocks, [])\n\ntiming_chain_len = 100\n\nbatch_sizes = [8, 80, 800, 8000, 64_000, 160_000, 320_000]\ntimes = []\nflips = []\ndofs = []\n\nschedule = SamplingSchedule(timing_chain_len, 1, 1)\n\ncall_f = jax.jit(\n    jax.vmap(\n        lambda k: sample_states(\n            k,\n            timing_program,\n            schedule,\n            [x[0] for x in init_state_free],\n            [],\n            [Block(nodes)],\n        )\n    )\n)\n\nfor batch_size in batch_sizes:\n    key, subkey = jax.random.split(key, 2)\n    keys = jax.random.split(key, batch_size)\n    keys = jax.device_put(keys, sharding)\n    _ = jax.block_until_ready(call_f(keys))\n\n    start_time = time.time()\n    _ = jax.block_until_ready(call_f(keys))\n    stop_time = time.time()\n\n    times.append(stop_time - start_time)\n    flips.append(timing_chain_len * len(nodes) * batch_size)\n    dofs.append(batch_size * len(nodes))\n</code></pre> <pre><code>flips_per_ns = [x / (y * 1e9) for x, y in zip(flips, times)]\n</code></pre> <pre><code>fig, axs = plt.subplots()\nplt.title(\"Performance on 8xB200\")\naxs.plot(dofs, flips_per_ns)\naxs.set_xscale(\"log\")\naxs.set_xlabel(\"Parallel Degrees of Freedom\")\naxs.set_ylabel(\"Flips/ns\")\nplt.savefig(\"fps.png\", dpi=300)\nplt.show()\n</code></pre> <p>You can compare your results to an FPGA implementation that bakes the sampling problem directly into hardware here (they get ~60 flips/ns).</p> <p>Note that despite our focus on quadratic models here, THRML comes with the ability to support spin interactions of arbitrary order out of the box. This ability can be accessed via <code>thrml.models.discrete_ebm.SpinEBMFactor</code>,</p> <pre><code># this creates a cubic interaction s_1 * s_2 * s_3 between a subset of our nodes\nSpinEBMFactor(\n    [Block(nodes[:10]), Block(nodes[10:20]), Block(nodes[20:30])],\n    jax.random.normal(key, (10,)),\n)\n</code></pre> <pre><code>SpinEBMFactor(\n  node_groups=[\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode()))\n  ],\n  weights=f32[10],\n  spin_node_groups=[\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode()))\n  ],\n  categorical_node_groups=[],\n  is_spin={thrml.pgm.SpinNode: True}\n)\n</code></pre> <p>That's about everything there is to know about binary EBMs in THRML! We hope you use these tools to help us gain a better understanding of how to most effectively use these powerful primitives in more advanced machine learning architectures.</p>"},{"location":"examples/03_parallel_tempering/","title":"Parallel Tempering with THRML-Boost","text":"<p>Standard block Gibbs sampling is fast, but it has a blind spot: multimodal distributions. When the energy landscape has multiple deep wells separated by high barriers, the sampler gets trapped in whichever mode it finds first and never escapes.</p> <p>This notebook shows the problem concretely, then demonstrates how parallel tempering fixes it \u2014 and how THRML-Boost runs all chains simultaneously via <code>jax.vmap</code> so you pay no extra compile cost for additional chains.</p> <pre><code>import time\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\n</code></pre> <pre><code>from thrml_boost.block_management import Block\nfrom thrml_boost.block_sampling import SamplingSchedule, sample_states\nfrom thrml_boost.models.ising import IsingEBM, IsingSamplingProgram, hinton_init\nfrom thrml_boost.pgm import SpinNode\nfrom thrml_boost.tempering import parallel_tempering\n</code></pre>"},{"location":"examples/03_parallel_tempering/#setup-a-strongly-ferromagnetic-ising-model","title":"Setup: a strongly ferromagnetic Ising model","text":"<p>We use a 10\u00d710 periodic Ising grid with strong ferromagnetic coupling and no bias. This model has two symmetric ground states: all spins up and all spins down, separated by a large energy barrier. It's a clean testbed for mixing.</p> <pre><code>side = 10\nG = nx.grid_2d_graph(side, side, periodic=True)\ncoord_to_node = {coord: SpinNode() for coord in G.nodes()}\nnx.relabel_nodes(G, coord_to_node, copy=False)\n\nnodes = list(G.nodes())\nedges = list(G.edges())\n\nbeta_target = 2.0\nbiases = jnp.zeros(len(nodes))\nweights = jnp.ones(len(edges)) * 1.0  # ferromagnetic\n\n# Two-color for block Gibbs\ncoloring = nx.bipartite.color(G)\ncolor0 = [n for n, c in coloring.items() if c == 0]\ncolor1 = [n for n, c in coloring.items() if c == 1]\nfree_blocks = [Block(color0), Block(color1)]\n\nkey = jax.random.key(42)\nprint(\n    f\"Nodes: {len(nodes)}, Edges: {len(edges)}, Block sizes: {len(color0)}, {len(color1)}\"\n)\n</code></pre> <pre><code>Nodes: 100, Edges: 200, Block sizes: 50, 50\n</code></pre>"},{"location":"examples/03_parallel_tempering/#the-problem-gibbs-gets-stuck","title":"The problem: Gibbs gets stuck","text":"<p>We track the mean magnetization \\(\\langle s \\rangle = \\frac{1}{N}\\sum_i s_i\\) over time. A sampler that mixes well should oscillate between \\(+1\\) (all up) and \\(-1\\) (all down). A stuck sampler stays near one value indefinitely.</p> <pre><code>ebm = IsingEBM(nodes, edges, biases, weights, jnp.array(beta_target))\nprogram = IsingSamplingProgram(ebm, free_blocks, clamped_blocks=[])\n\nkey, subkey = jax.random.split(key)\ninit_state = hinton_init(subkey, ebm, free_blocks, ())\n\nschedule = SamplingSchedule(n_warmup=50, n_samples=400, steps_per_sample=2)\nkey, subkey = jax.random.split(key)\ngibbs_samples = sample_states(subkey, program, schedule, init_state, [], [Block(nodes)])\n\nspins = 2 * gibbs_samples[0].astype(jnp.int8) - 1\nmag_gibbs = jnp.mean(spins, axis=-1)\n\nplt.figure(figsize=(12, 3))\nplt.plot(mag_gibbs, lw=0.8, color=\"steelblue\")\nplt.axhline(0, color=\"k\", lw=0.5, linestyle=\"--\")\nplt.fill_between(range(len(mag_gibbs)), -1, 1, alpha=0.03, color=\"gray\")\nplt.ylim(-1.1, 1.1)\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Mean magnetization\")\nplt.title(f\"Standard Gibbs at \u03b2={beta_target} \u2014 trapped in one mode\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/03_parallel_tempering/#the-fix-parallel-tempering","title":"The fix: parallel tempering","text":"<p>Parallel tempering runs multiple chains at different temperatures simultaneously:</p> <ul> <li>Hot chains (small \u03b2) \u2014 energy barriers are flattened, the chain explores freely</li> <li>Cold chains (large \u03b2) \u2014 accurate samples from the target distribution</li> <li>Every round, adjacent chains propose a swap. If accepted, the cold chain receives a configuration the hot chain found on the other side of a barrier.</li> </ul> <p>But this only works if the temperature ladder is tuned correctly. If adjacent chains are too different, swaps are almost never accepted and the chains don't communicate. Let's see what that looks like.</p> <pre><code># A naive beta ladder \u2014 large gap at the bottom, crowded at the top\nbetas_naive = [0.1, 0.3, 0.7, 1.2, beta_target]\nn_chains = len(betas_naive)\n\nebms_naive = [\n    IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas_naive\n]\nprograms_naive = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_naive]\n\nkey, subkey = jax.random.split(key)\ninit = hinton_init(subkey, ebms_naive[0], free_blocks, ())\n\n\n@jax.jit\ndef one_round_naive(key, states):\n    return parallel_tempering(\n        key,\n        ebms_naive,\n        programs_naive,\n        states,\n        [],\n        n_rounds=1,\n        gibbs_steps_per_round=2,\n    )\n\n\nmag_naive = []\nstates_naive = [init] * n_chains\nkey, subkey = jax.random.split(key)\nfor k in jax.random.split(subkey, 400):\n    states_naive, _, stats_naive = one_round_naive(k, states_naive)\n    cold = jnp.concatenate(states_naive[-1])\n    mag_naive.append(float(jnp.mean(2 * cold.astype(jnp.int8) - 1)))\n\nprint(\"Swap acceptance rates with naive ladder:\")\nfor i, r in enumerate(stats_naive[\"acceptance_rate\"]):\n    flag = \"\u2713\" if 0.2 &lt;= float(r) &lt;= 0.5 else \"\u2717\"\n    print(\n        f\"  {flag} \u03b2={betas_naive[i]:.1f} \u2194 \u03b2={betas_naive[i + 1]:.1f}: {float(r):.1%}\"\n    )\n</code></pre> <pre><code>Swap acceptance rates with naive ladder:\n  \u2717 \u03b2=0.1 \u2194 \u03b2=0.3: 0.0%\n  \u2717 \u03b2=0.3 \u2194 \u03b2=0.7: 0.0%\n  \u2717 \u03b2=0.7 \u2194 \u03b2=1.2: 0.0%\n  \u2717 \u03b2=1.2 \u2194 \u03b2=2.0: 0.0%\n</code></pre> <p>The large gap between \u03b2=0.1 and \u03b2=0.3 gives near-zero swap acceptance \u2014 those chains never communicate, so the cold chain still can't escape its mode.</p>"},{"location":"examples/03_parallel_tempering/#tuning-the-temperature-ladder","title":"Tuning the temperature ladder","text":"<p>We need acceptance rates in the 20\u201350% range on every pair. The iterative refinement below starts from a coarse geometric ladder and repeatedly inserts a new temperature at the geometric midpoint of the worst-performing pair until convergence.</p> <pre><code>def run_acceptance(betas, key, n_rounds=300):\n    n = len(betas)\n    ebms_t = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas]\n    progs_t = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_t]\n    key, subkey = jax.random.split(key)\n    init_t = hinton_init(subkey, ebms_t[0], free_blocks, ())\n\n    @jax.jit\n    def _run(key, states):\n        return parallel_tempering(\n            key, ebms_t, progs_t, states, [], n_rounds=n_rounds, gibbs_steps_per_round=2\n        )\n\n    key, subkey = jax.random.split(key)\n    _, _, stats = _run(subkey, [init_t] * n)\n    return np.array(stats[\"acceptance_rate\"])\n\n\ndef refine_ladder(betas, key, min_rate=0.2, max_iters=8):\n    betas = sorted(list(betas))\n    for i in range(max_iters):\n        key, subkey = jax.random.split(key)\n        rates = run_acceptance(betas, subkey)\n        worst = int(np.argmin(rates))\n        worst_rate = rates[worst]\n        print(\n            f\"  Iter {i + 1}: {len(betas)} chains, \"\n            f\"min rate = {worst_rate:.1%} at \u03b2={betas[worst]:.3f}\u2194{betas[worst + 1]:.3f}\"\n        )\n        if worst_rate &gt;= min_rate:\n            print(f\"  All pairs above {min_rate:.0%} \u2014 converged.\")\n            break\n        new_beta = float(np.sqrt(betas[worst] * betas[worst + 1]))\n        betas.insert(worst + 1, new_beta)\n    return betas, rates\n\n\nprint(\"Refining from a 5-chain geometric ladder...\")\nstart_betas = np.geomspace(0.1, beta_target, 5).tolist()\nkey, subkey = jax.random.split(key)\nrefined_betas, _ = refine_ladder(start_betas, subkey)\nprint(f\"\\nFinal ladder: {[f'{b:.3f}' for b in refined_betas]}\")\n</code></pre> <pre><code>Refining from a 5-chain geometric ladder...\n  Iter 1: 5 chains, min rate = 0.0% at \u03b2=0.211\u21940.447\n  Iter 2: 6 chains, min rate = 1.3% at \u03b2=0.308\u21940.447\n  Iter 3: 7 chains, min rate = 1.3% at \u03b2=0.447\u21940.946\n  Iter 4: 8 chains, min rate = 8.0% at \u03b2=0.447\u21940.650\n  Iter 5: 9 chains, min rate = 10.7% at \u03b2=0.371\u21940.447\n  Iter 6: 10 chains, min rate = 20.7% at \u03b2=0.447\u21940.539\n  All pairs above 20% \u2014 converged.\n\nFinal ladder: ['0.100', '0.211', '0.308', '0.371', '0.407', '0.447', '0.539', '0.650', '0.946', '2.000']\n</code></pre> <pre><code># Plot final acceptance rates\nkey, subkey = jax.random.split(key)\nfinal_rates = run_acceptance(refined_betas, subkey)\npair_labels = [\n    f\"{refined_betas[i]:.2f}\u2194{refined_betas[i + 1]:.2f}\"\n    for i in range(len(refined_betas) - 1)\n]\nbar_colors = [\n    \"green\" if 0.2 &lt;= r &lt;= 0.5 else \"orange\" if r &gt; 0.5 else \"red\" for r in final_rates\n]\n\nplt.figure(figsize=(9, 3))\nplt.bar(pair_labels, final_rates, color=bar_colors, edgecolor=\"white\")\nplt.axhline(0.2, color=\"k\", lw=1, linestyle=\"--\", label=\"20% lower target\")\nplt.axhline(0.5, color=\"k\", lw=1, linestyle=\":\", label=\"50% upper target\")\nplt.ylabel(\"Acceptance rate\")\nplt.title(f\"Refined ladder \u2014 {len(refined_betas)} chains\")\nplt.xticks(rotation=25, ha=\"right\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/03_parallel_tempering/#comparison-naive-vs-tuned-ladder","title":"Comparison: naive vs tuned ladder","text":"<p>Now we can see the real benefit. With a well-tuned ladder, the cold chain mixes freely between both modes.</p> <pre><code># Run PT with refined betas\nebms_refined = [\n    IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in refined_betas\n]\nprograms_refined = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_refined]\nn_ref = len(refined_betas)\n\nkey, subkey = jax.random.split(key)\ninit_ref = hinton_init(subkey, ebms_refined[0], free_blocks, ())\n\n\n@jax.jit\ndef one_round_refined(key, states):\n    return parallel_tempering(\n        key,\n        ebms_refined,\n        programs_refined,\n        states,\n        [],\n        n_rounds=1,\n        gibbs_steps_per_round=2,\n    )\n\n\nmag_refined = []\nstates_ref = [init_ref] * n_ref\nkey, subkey = jax.random.split(key)\nfor k in jax.random.split(subkey, 400):\n    states_ref, _, _ = one_round_refined(k, states_ref)\n    cold = jnp.concatenate(states_ref[-1])\n    mag_refined.append(float(jnp.mean(2 * cold.astype(jnp.int8) - 1)))\n\n# Three-way comparison\nfig, axes = plt.subplots(3, 1, figsize=(12, 7), sharex=True)\n\nfor ax, mag, color, title in zip(\n    axes,\n    [mag_gibbs, mag_naive, mag_refined],\n    [\"steelblue\", \"orangered\", \"green\"],\n    [\n        \"Standard Gibbs \u2014 trapped\",\n        f\"PT with naive betas {[round(b, 1) for b in betas_naive]} \u2014 still trapped\",\n        f\"PT with refined betas ({len(refined_betas)} chains) \u2014 mixing between modes\",\n    ],\n):\n    ax.plot(mag, lw=0.8, color=color)\n    ax.axhline(0, color=\"k\", lw=0.5, linestyle=\"--\")\n    ax.set_ylim(-1.1, 1.1)\n    ax.set_ylabel(\"Magnetization\")\n    ax.set_title(title)\n\naxes[-1].set_xlabel(\"Round\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"examples/03_parallel_tempering/#performance-compile-time-stays-flat-with-more-chains","title":"Performance: compile time stays flat with more chains","text":"<p>In the original thrml, more chains meant longer compile time because each chain was unrolled separately into the XLA graph. With <code>jax.vmap</code> in THRML-Boost, compile time is essentially constant \u2014 you only pay in runtime, which scales linearly.</p> <pre><code>chain_counts = [2, 4, 8, 16]\ncompile_times = []\n\nfor n in chain_counts:\n    betas_n = np.geomspace(0.1, beta_target, n).tolist()\n    ebms_n = [IsingEBM(nodes, edges, biases, weights, jnp.array(b)) for b in betas_n]\n    progs_n = [IsingSamplingProgram(e, free_blocks, []) for e in ebms_n]\n    key, subkey = jax.random.split(key)\n    init_n = hinton_init(subkey, ebms_n[0], free_blocks, ())\n\n    @jax.jit\n    def _run(key, states):\n        return parallel_tempering(\n            key, ebms_n, progs_n, states, [], n_rounds=5, gibbs_steps_per_round=1\n        )\n\n    key, subkey = jax.random.split(key)\n    t0 = time.time()\n    result = _run(subkey, [init_n] * n)\n    jax.block_until_ready(result[0])\n    compile_times.append(time.time() - t0)\n    print(f\"  {n:2d} chains: {compile_times[-1]:.2f}s\")\n\nplt.figure(figsize=(6, 3))\nplt.plot(\n    chain_counts, compile_times, \"o-\", color=\"steelblue\", label=\"THRML-Boost (vmap)\"\n)\nplt.xlabel(\"Number of chains\")\nplt.ylabel(\"First-call time (compile + run, s)\")\nplt.title(\"Compile time is flat across chain counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>2 chains: 0.30s\n   4 chains: 0.46s\n   8 chains: 0.73s\n  16 chains: 1.29s\n</code></pre> <p></p>"},{"location":"examples/03_parallel_tempering/#summary","title":"Summary","text":"<p>Parallel tempering in THRML-Boost:</p> <ul> <li>Escapes multimodal traps that standard Gibbs cannot</li> <li>All chains run in a single <code>jax.vmap</code> kernel \u2014 no compile cost for extra chains</li> <li>Swap statistics guide temperature ladder tuning</li> <li>The iterative refinement loop above handles problem-specific structure automatically</li> </ul> <pre><code>final_states, sampler_states, stats = parallel_tempering(\n    key,\n    ebms,            # one IsingEBM per temperature\n    programs,        # one IsingSamplingProgram per temperature\n    init_states,     # one initial state per temperature\n    clamp_state=[],\n    n_rounds=500,\n    gibbs_steps_per_round=2,\n)\n\n# Check your ladder\nprint(stats['acceptance_rate'])   # aim for 0.2\u20130.5 on every pair\n</code></pre>"}]}